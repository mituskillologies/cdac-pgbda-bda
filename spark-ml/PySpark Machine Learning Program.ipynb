{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYv8XtUp24WNWyUj2/XxOV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"P-qvoXSBMQyx","executionInfo":{"status":"ok","timestamp":1675394457422,"user_tz":-330,"elapsed":51514,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"439dd7df-9cc0-4833-9b82-cb6ed2781891"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=d88383bc530934812fe7148d01e987529dcd0bba232bfdb5e5c35ec556470771\n","  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["py4j","pyspark"]}}},"metadata":{}}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"id":"yM564tyrMTYS","executionInfo":{"status":"ok","timestamp":1675394482616,"user_tz":-330,"elapsed":10,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["spark = SparkSession.builder.master('local[4]').appName('ml').getOrCreate()"],"metadata":{"id":"H2GorH0XSUSl","executionInfo":{"status":"ok","timestamp":1675394498118,"user_tz":-330,"elapsed":11316,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df = spark.read.csv('diabetes.csv', header= True, inferSchema=True)"],"metadata":{"id":"_cvUVVpiSYBq","executionInfo":{"status":"ok","timestamp":1675394520697,"user_tz":-330,"elapsed":12704,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["sc = spark.sparkContext"],"metadata":{"id":"5IQt9_hszuqh","executionInfo":{"status":"ok","timestamp":1675403155062,"user_tz":-330,"elapsed":805,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgN0la98S2BA","executionInfo":{"status":"ok","timestamp":1675394535037,"user_tz":-330,"elapsed":19,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"0e0b4d31-0c70-4c08-ca7e-81ab1018d9a8"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[Pregnancies: int, Glucose: int, BloodPressure: int, SkinThickness: int, Insulin: int, BMI: double, Pedigree: double, Age: int, Outcome: int]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Jz9gSRcS7cy","executionInfo":{"status":"ok","timestamp":1675394616218,"user_tz":-330,"elapsed":1349,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"36b61a6f-1941-4bf2-c50b-51a639f264a7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJWnJ5RdTPy-","executionInfo":{"status":"ok","timestamp":1675394671710,"user_tz":-330,"elapsed":6,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"b93c81e6-4ead-4215-ad0c-0bd1ea6259c9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-------+-------------+-------------+-------+----+--------+---+-------+\n","|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|Pedigree|Age|Outcome|\n","+-----------+-------+-------------+-------------+-------+----+--------+---+-------+\n","|          6|    148|           72|           35|      0|33.6|   0.627| 50|      1|\n","|          1|     85|           66|           29|      0|26.6|   0.351| 31|      0|\n","|          8|    183|           64|            0|      0|23.3|   0.672| 32|      1|\n","|          1|     89|           66|           23|     94|28.1|   0.167| 21|      0|\n","|          0|    137|           40|           35|    168|43.1|   2.288| 33|      1|\n","+-----------+-------+-------------+-------------+-------+----+--------+---+-------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["df.summary().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38ZRgDdxTXuL","executionInfo":{"status":"ok","timestamp":1675394756914,"user_tz":-330,"elapsed":2750,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"35e837ed-015c-4e21-fdfb-54a7167a3d48"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n","|summary|       Pregnancies|          Glucose|     BloodPressure|     SkinThickness|           Insulin|               BMI|          Pedigree|               Age|           Outcome|\n","+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n","|  count|               768|              768|               768|               768|               768|               768|               768|               768|               768|\n","|   mean|3.8450520833333335|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.992578124999977|0.4718763020833327|33.240885416666664|0.3489583333333333|\n","| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803| 7.884160320375441| 0.331328595012775|11.760231540678689| 0.476951377242799|\n","|    min|                 0|                0|                 0|                 0|                 0|               0.0|             0.078|                21|                 0|\n","|    25%|                 1|               99|                62|                 0|                 0|              27.3|             0.243|                24|                 0|\n","|    50%|                 3|              117|                72|                23|                29|              32.0|             0.371|                29|                 0|\n","|    75%|                 6|              140|                80|                32|               127|              36.6|             0.626|                41|                 1|\n","|    max|                17|              199|               122|                99|               846|              67.1|              2.42|                81|                 1|\n","+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n","\n"]}]},{"cell_type":"code","source":["pdf = df.toPandas()"],"metadata":{"id":"jkVbbOK2TvIk","executionInfo":{"status":"ok","timestamp":1675394877002,"user_tz":-330,"elapsed":877,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["pdf.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NujnmP9iUNQQ","executionInfo":{"status":"ok","timestamp":1675394888278,"user_tz":-330,"elapsed":610,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"0bfe0fa4-1c5d-4d4a-db48-222a0f3f9780"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 768 entries, 0 to 767\n","Data columns (total 9 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   Pregnancies    768 non-null    int32  \n"," 1   Glucose        768 non-null    int32  \n"," 2   BloodPressure  768 non-null    int32  \n"," 3   SkinThickness  768 non-null    int32  \n"," 4   Insulin        768 non-null    int32  \n"," 5   BMI            768 non-null    float64\n"," 6   Pedigree       768 non-null    float64\n"," 7   Age            768 non-null    int32  \n"," 8   Outcome        768 non-null    int32  \n","dtypes: float64(2), int32(7)\n","memory usage: 33.1 KB\n"]}]},{"cell_type":"code","source":["# Check the balance of data\n","df.groupby('Outcome').count().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BiwzcqYFUST2","executionInfo":{"status":"ok","timestamp":1675394999009,"user_tz":-330,"elapsed":800,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"aa2ff8c1-eba4-4ad4-8c22-bc2180613aa9"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-----+\n","|Outcome|count|\n","+-------+-----+\n","|      1|  268|\n","|      0|  500|\n","+-------+-----+\n","\n"]}]},{"cell_type":"code","source":["pdf['Outcome'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NInaB9ZFUtgT","executionInfo":{"status":"ok","timestamp":1675395136931,"user_tz":-330,"elapsed":7,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"7d7df4bd-9664-4ead-d5fe-a646326c7631"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    500\n","1    268\n","Name: Outcome, dtype: int64"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["pdf['Outcome'].value_counts().plot.bar()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"v-1PAbIRVLao","executionInfo":{"status":"ok","timestamp":1675395226752,"user_tz":-330,"elapsed":1346,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"ea29918c-6836-4505-8e8b-3638bf9c46ea"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f3b2f1ffeb0>"]},"metadata":{},"execution_count":17},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["import seaborn as sns\n","sns.countplot(x = pdf['Outcome'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"-hShUfykVlG7","executionInfo":{"status":"ok","timestamp":1675395287304,"user_tz":-330,"elapsed":29,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"45305dff-6228-46fb-bd37-33ac2a5c8117"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f3b2743bf10>"]},"metadata":{},"execution_count":19},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["round(pdf['Outcome'].value_counts()[0] / len(pdf), 2) * 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txtpEl0vVydc","executionInfo":{"status":"ok","timestamp":1675395724499,"user_tz":-330,"elapsed":9,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"0cfc304b-9ca3-49b9-8c1c-066d6bf7545a"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["65.0"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["round(pdf['Outcome'].value_counts()[1] / len(pdf), 2) * 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hc4teMm6XUov","executionInfo":{"status":"ok","timestamp":1675395734126,"user_tz":-330,"elapsed":13,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"4445606a-2343-4252-e796-73d99ea93e22"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35.0"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["pdf.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGpO-NM-XhQ-","executionInfo":{"status":"ok","timestamp":1675395889278,"user_tz":-330,"elapsed":18,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"a11a72ce-4496-4f3d-87f4-b12854a3e687"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pregnancies      0\n","Glucose          0\n","BloodPressure    0\n","SkinThickness    0\n","Insulin          0\n","BMI              0\n","Pedigree         0\n","Age              0\n","Outcome          0\n","dtype: int64"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["from pyspark.sql.functions import isnull, when, count, col"],"metadata":{"id":"jvxh61MGYAUW","executionInfo":{"status":"ok","timestamp":1675395973463,"user_tz":-330,"elapsed":12,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# no missing values\n","df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKx--kHRYbcX","executionInfo":{"status":"ok","timestamp":1675395994071,"user_tz":-330,"elapsed":1934,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"518359f3-62c3-49ff-990d-72f1d9cb58b4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-------+-------------+-------------+-------+---+--------+---+-------+\n","|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin|BMI|Pedigree|Age|Outcome|\n","+-----------+-------+-------------+-------------+-------+---+--------+---+-------+\n","|          0|      0|            0|            0|      0|  0|       0|  0|      0|\n","+-----------+-------+-------------+-------------+-------+---+--------+---+-------+\n","\n"]}]},{"cell_type":"code","source":["# Assemble the features\n","required_features = ['Glucose','BloodPressure','BMI','Age']"],"metadata":{"id":"z9nu7DJdYf_L","executionInfo":{"status":"ok","timestamp":1675396283568,"user_tz":-330,"elapsed":675,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["pdf[required_features]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"kFJZbb7NZnKh","executionInfo":{"status":"ok","timestamp":1675396983757,"user_tz":-330,"elapsed":629,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"d9e0c063-c99c-44f9-8658-75d3f4f85ad4"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Glucose  BloodPressure   BMI  Age\n","0        148             72  33.6   50\n","1         85             66  26.6   31\n","2        183             64  23.3   32\n","3         89             66  28.1   21\n","4        137             40  43.1   33\n","..       ...            ...   ...  ...\n","763      101             76  32.9   63\n","764      122             70  36.8   27\n","765      121             72  26.2   30\n","766      126             60  30.1   47\n","767       93             70  30.4   23\n","\n","[768 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-d3616d66-4f10-46be-904a-897106be08ea\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>BMI</th>\n","      <th>Age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>33.6</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>26.6</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>23.3</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>28.1</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>43.1</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>101</td>\n","      <td>76</td>\n","      <td>32.9</td>\n","      <td>63</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>122</td>\n","      <td>70</td>\n","      <td>36.8</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>765</th>\n","      <td>121</td>\n","      <td>72</td>\n","      <td>26.2</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>766</th>\n","      <td>126</td>\n","      <td>60</td>\n","      <td>30.1</td>\n","      <td>47</td>\n","    </tr>\n","    <tr>\n","      <th>767</th>\n","      <td>93</td>\n","      <td>70</td>\n","      <td>30.4</td>\n","      <td>23</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>768 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3616d66-4f10-46be-904a-897106be08ea')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d3616d66-4f10-46be-904a-897106be08ea button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d3616d66-4f10-46be-904a-897106be08ea');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["# import the vector assembler class\n","from pyspark.ml.feature import VectorAssembler"],"metadata":{"id":"MeJvEvhYcSM5","executionInfo":{"status":"ok","timestamp":1675397076097,"user_tz":-330,"elapsed":13,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# create the object\n","assembler = VectorAssembler(inputCols=required_features,\n","                            outputCol='features')"],"metadata":{"id":"Edx481FXco3K","executionInfo":{"status":"ok","timestamp":1675397087147,"user_tz":-330,"elapsed":6,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["transformed_data = assembler.transform(df)"],"metadata":{"id":"7Z1R3qJycrkc","executionInfo":{"status":"ok","timestamp":1675397118298,"user_tz":-330,"elapsed":5,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["transformed_data.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6BvWxK6czJl","executionInfo":{"status":"ok","timestamp":1675397143493,"user_tz":-330,"elapsed":2170,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"bb9a2699-ea43-488c-f0d3-3d54288f7ddb"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-------+-------------+-------------+-------+----+--------+---+-------+--------------------+\n","|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|Pedigree|Age|Outcome|            features|\n","+-----------+-------+-------------+-------------+-------+----+--------+---+-------+--------------------+\n","|          6|    148|           72|           35|      0|33.6|   0.627| 50|      1|[148.0,72.0,33.6,...|\n","|          1|     85|           66|           29|      0|26.6|   0.351| 31|      0|[85.0,66.0,26.6,3...|\n","|          8|    183|           64|            0|      0|23.3|   0.672| 32|      1|[183.0,64.0,23.3,...|\n","|          1|     89|           66|           23|     94|28.1|   0.167| 21|      0|[89.0,66.0,28.1,2...|\n","|          0|    137|           40|           35|    168|43.1|   2.288| 33|      1|[137.0,40.0,43.1,...|\n","+-----------+-------+-------------+-------------+-------+----+--------+---+-------+--------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"If4xlzvFy0md"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the data in train and test\n","training_data, test_data = transformed_data.randomSplit([0.75, 0.25],\n","                                                        seed = 0)"],"metadata":{"id":"LrXAsO4gc4yp","executionInfo":{"status":"ok","timestamp":1675397390724,"user_tz":-330,"elapsed":1206,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["training_data.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWP8Z3A9d1lo","executionInfo":{"status":"ok","timestamp":1675397793588,"user_tz":-330,"elapsed":2618,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"81c34da0-3d74-4285-997c-7e35ddb07dec"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["573"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["test_data.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-y81448fXmb","executionInfo":{"status":"ok","timestamp":1675397803872,"user_tz":-330,"elapsed":2629,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"c139fc5c-064f-4780-c3c0-9eea9f051c43"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["195"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["training_data.groupby('Outcome').count().select('count')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5nRr07OSfZ_S","executionInfo":{"status":"ok","timestamp":1675398424009,"user_tz":-330,"elapsed":1037,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"f08aaacf-9c19-4f70-a853-500fcd7ee5c5"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[count: bigint]"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["test_data.groupby('Outcome').count().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzjnPVRSfrqu","executionInfo":{"status":"ok","timestamp":1675397911916,"user_tz":-330,"elapsed":1548,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"4396c14e-06c5-4fff-f596-9ee0d94f9264"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-----+\n","|Outcome|count|\n","+-------+-----+\n","|      1|   78|\n","|      0|  117|\n","+-------+-----+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier"],"metadata":{"id":"jzjk_2tlfyzi","executionInfo":{"status":"ok","timestamp":1675398470263,"user_tz":-330,"elapsed":748,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["dt = DecisionTreeClassifier(labelCol='Outcome', featuresCol='features')"],"metadata":{"id":"4PXdzrlbh9Ru","executionInfo":{"status":"ok","timestamp":1675398512257,"user_tz":-330,"elapsed":701,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# train the algorithm\n","model = dt.fit(training_data)"],"metadata":{"id":"_X3hELqhiHgs","executionInfo":{"status":"ok","timestamp":1675399605221,"user_tz":-330,"elapsed":9464,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["# predictions on unseen data\n","predictions = model.transform(test_data)"],"metadata":{"id":"K1aElXrzmQO6","executionInfo":{"status":"ok","timestamp":1675399626386,"user_tz":-330,"elapsed":852,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["predictions.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Z0FHidImXSP","executionInfo":{"status":"ok","timestamp":1675399782757,"user_tz":-330,"elapsed":1691,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"2581bc39-42a6-4b55-ff1e-8cfc3a06a38a"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-------+-------------+-------------+-------+----+--------+---+-------+--------------------+-------------+--------------------+----------+\n","|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|Pedigree|Age|Outcome|            features|rawPrediction|         probability|prediction|\n","+-----------+-------+-------------+-------------+-------+----+--------+---+-------+--------------------+-------------+--------------------+----------+\n","|          0|     57|           60|            0|      0|21.7|   0.735| 67|      0|[57.0,60.0,21.7,6...|  [65.0,16.0]|[0.80246913580246...|       0.0|\n","|          0|     99|            0|            0|      0|25.0|   0.253| 22|      0|[99.0,0.0,25.0,22.0]|  [117.0,1.0]|[0.99152542372881...|       0.0|\n","|          0|    100|           70|           26|     50|30.8|   0.597| 21|      0|[100.0,70.0,30.8,...|  [117.0,1.0]|[0.99152542372881...|       0.0|\n","|          0|    102|           64|           46|     78|40.6|   0.496| 21|      0|[102.0,64.0,40.6,...|   [61.0,6.0]|[0.91044776119402...|       0.0|\n","|          0|    102|           75|           23|      0| 0.0|   0.572| 21|      0|[102.0,75.0,0.0,2...|  [117.0,1.0]|[0.99152542372881...|       0.0|\n","+-----------+-------+-------------+-------------+-------+----+--------+---+-------+--------------------+-------------+--------------------+----------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["pred = predictions.select('prediction').toPandas()"],"metadata":{"id":"dwPyEVAim5JT","executionInfo":{"status":"ok","timestamp":1675400174415,"user_tz":-330,"elapsed":1779,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["actual = test_data.select('Outcome').toPandas()"],"metadata":{"id":"xuz9mpnnoOS0","executionInfo":{"status":"ok","timestamp":1675400165906,"user_tz":-330,"elapsed":1599,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report"],"metadata":{"id":"8gjjWcgInN2l","executionInfo":{"status":"ok","timestamp":1675400076033,"user_tz":-330,"elapsed":782,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["ConfusionMatrixDisplay.from_predictions(actual, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"ycbUB-R0oE47","executionInfo":{"status":"ok","timestamp":1675400182433,"user_tz":-330,"elapsed":826,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"659515a9-e23a-45a2-a626-316385205da9"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3b36c56dc0>"]},"metadata":{},"execution_count":67},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["accuracy_score(actual, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPdP0Hl8ofRL","executionInfo":{"status":"ok","timestamp":1675402462958,"user_tz":-330,"elapsed":861,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"7444560f-0bfb-4fa9-8652-d0ee91d521cc"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7333333333333333"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["print(classification_report(actual, pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ro1wUlYxL2G","executionInfo":{"status":"ok","timestamp":1675402472657,"user_tz":-330,"elapsed":544,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"d6d5ee7d-d9f2-4567-f0a2-bbd5ebf373fd"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.81      0.79       117\n","           1       0.69      0.62      0.65        78\n","\n","    accuracy                           0.73       195\n","   macro avg       0.72      0.71      0.72       195\n","weighted avg       0.73      0.73      0.73       195\n","\n"]}]},{"cell_type":"code","source":["# import the evaluation class\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{"id":"aYA4RYCDxOd9","executionInfo":{"status":"ok","timestamp":1675402604744,"user_tz":-330,"elapsed":1094,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["eval = MulticlassClassificationEvaluator(labelCol='Outcome',\n","                                         metricName='accuracy')"],"metadata":{"id":"7bVnbuHBxuCZ","executionInfo":{"status":"ok","timestamp":1675402614593,"user_tz":-330,"elapsed":1023,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["print('Accuracy:', eval.evaluate(predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqgVMRH0xw7j","executionInfo":{"status":"ok","timestamp":1675402626277,"user_tz":-330,"elapsed":786,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"c6cb3858-0a68-4016-8b79-beb5209c7a9c"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7333333333333333\n"]}]},{"cell_type":"code","source":["new = sc.parallelize([89,66,25.2,34])  # New person"],"metadata":{"id":"42RwyqCxxz1f","executionInfo":{"status":"ok","timestamp":1675403162240,"user_tz":-330,"elapsed":731,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["import pyspark\n","help(pyspark.ml.classification)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_At3Kz43zkeH","executionInfo":{"status":"ok","timestamp":1675404191859,"user_tz":-330,"elapsed":1816,"user":{"displayName":"Tushar B. Kute","userId":"12883343659087939127"}},"outputId":"da4dcfac-6878-4baf-abc9-77f1ac03cf38"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on module pyspark.ml.classification in pyspark.ml:\n","\n","NAME\n","    pyspark.ml.classification\n","\n","DESCRIPTION\n","    # Licensed to the Apache Software Foundation (ASF) under one or more\n","    # contributor license agreements.  See the NOTICE file distributed with\n","    # this work for additional information regarding copyright ownership.\n","    # The ASF licenses this file to You under the Apache License, Version 2.0\n","    # (the \"License\"); you may not use this file except in compliance with\n","    # the License.  You may obtain a copy of the License at\n","    #\n","    #    http://www.apache.org/licenses/LICENSE-2.0\n","    #\n","    # Unless required by applicable law or agreed to in writing, software\n","    # distributed under the License is distributed on an \"AS IS\" BASIS,\n","    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","    # See the License for the specific language governing permissions and\n","    # limitations under the License.\n","    #\n","\n","CLASSES\n","    pyspark.ml.base.Estimator(pyspark.ml.param.Params, typing.Generic)\n","        OneVsRest(pyspark.ml.base.Estimator, _OneVsRestParams, pyspark.ml.param.shared.HasParallelism, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable, typing.Generic)\n","    pyspark.ml.base.Model(pyspark.ml.base.Transformer)\n","        OneVsRestModel(pyspark.ml.base.Model, _OneVsRestParams, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable)\n","    _BinaryClassificationSummary(_ClassificationSummary)\n","        BinaryLogisticRegressionSummary(_BinaryClassificationSummary, LogisticRegressionSummary)\n","            BinaryLogisticRegressionTrainingSummary(BinaryLogisticRegressionSummary, LogisticRegressionTrainingSummary)\n","        BinaryRandomForestClassificationSummary\n","            BinaryRandomForestClassificationTrainingSummary(BinaryRandomForestClassificationSummary, RandomForestClassificationTrainingSummary)\n","        FMClassificationSummary\n","            FMClassificationTrainingSummary(FMClassificationSummary, _TrainingSummary)\n","        LinearSVCSummary\n","            LinearSVCTrainingSummary(LinearSVCSummary, _TrainingSummary)\n","    _ClassificationSummary(pyspark.ml.wrapper.JavaWrapper)\n","        LogisticRegressionSummary\n","            BinaryLogisticRegressionSummary(_BinaryClassificationSummary, LogisticRegressionSummary)\n","                BinaryLogisticRegressionTrainingSummary(BinaryLogisticRegressionSummary, LogisticRegressionTrainingSummary)\n","            LogisticRegressionTrainingSummary(LogisticRegressionSummary, _TrainingSummary)\n","        MultilayerPerceptronClassificationSummary\n","            MultilayerPerceptronClassificationTrainingSummary(MultilayerPerceptronClassificationSummary, _TrainingSummary)\n","        RandomForestClassificationSummary\n","            RandomForestClassificationTrainingSummary(RandomForestClassificationSummary, _TrainingSummary)\n","    _DecisionTreeClassifierParams(pyspark.ml.tree._DecisionTreeParams, pyspark.ml.tree._TreeClassifierParams)\n","        DecisionTreeClassificationModel(pyspark.ml.tree._DecisionTreeModel, _JavaProbabilisticClassificationModel, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        DecisionTreeClassifier(_JavaProbabilisticClassifier, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    _GBTClassifierParams(pyspark.ml.tree._GBTParams, pyspark.ml.tree._HasVarianceImpurity)\n","        GBTClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        GBTClassifier(_JavaProbabilisticClassifier, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    _JavaClassificationModel(ClassificationModel, pyspark.ml.wrapper.JavaPredictionModel)\n","        LinearSVCModel(_JavaClassificationModel, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","    _JavaClassifier(Classifier, pyspark.ml.wrapper.JavaPredictor, typing.Generic)\n","        LinearSVC(_JavaClassifier, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    _JavaProbabilisticClassificationModel(ProbabilisticClassificationModel, _JavaClassificationModel)\n","        DecisionTreeClassificationModel(pyspark.ml.tree._DecisionTreeModel, _JavaProbabilisticClassificationModel, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        FMClassificationModel(_JavaProbabilisticClassificationModel, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        GBTClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LogisticRegressionModel(_JavaProbabilisticClassificationModel, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        MultilayerPerceptronClassificationModel(_JavaProbabilisticClassificationModel, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        NaiveBayesModel(_JavaProbabilisticClassificationModel, _NaiveBayesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        RandomForestClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","    _JavaProbabilisticClassifier(ProbabilisticClassifier, _JavaClassifier, typing.Generic)\n","        DecisionTreeClassifier(_JavaProbabilisticClassifier, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        FMClassifier(_JavaProbabilisticClassifier, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        GBTClassifier(_JavaProbabilisticClassifier, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LogisticRegression(_JavaProbabilisticClassifier, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        MultilayerPerceptronClassifier(_JavaProbabilisticClassifier, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        NaiveBayes(_JavaProbabilisticClassifier, _NaiveBayesParams, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        RandomForestClassifier(_JavaProbabilisticClassifier, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    _LinearSVCParams(_ClassifierParams, pyspark.ml.param.shared.HasRegParam, pyspark.ml.param.shared.HasMaxIter, pyspark.ml.param.shared.HasFitIntercept, pyspark.ml.param.shared.HasTol, pyspark.ml.param.shared.HasStandardization, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.param.shared.HasAggregationDepth, pyspark.ml.param.shared.HasThreshold, pyspark.ml.param.shared.HasMaxBlockSizeInMB)\n","        LinearSVC(_JavaClassifier, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LinearSVCModel(_JavaClassificationModel, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","    _LogisticRegressionParams(_ProbabilisticClassifierParams, pyspark.ml.param.shared.HasRegParam, pyspark.ml.param.shared.HasElasticNetParam, pyspark.ml.param.shared.HasMaxIter, pyspark.ml.param.shared.HasFitIntercept, pyspark.ml.param.shared.HasTol, pyspark.ml.param.shared.HasStandardization, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.param.shared.HasAggregationDepth, pyspark.ml.param.shared.HasThreshold, pyspark.ml.param.shared.HasMaxBlockSizeInMB)\n","        LogisticRegression(_JavaProbabilisticClassifier, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LogisticRegressionModel(_JavaProbabilisticClassificationModel, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","    _MultilayerPerceptronParams(_ProbabilisticClassifierParams, pyspark.ml.param.shared.HasSeed, pyspark.ml.param.shared.HasMaxIter, pyspark.ml.param.shared.HasTol, pyspark.ml.param.shared.HasStepSize, pyspark.ml.param.shared.HasSolver, pyspark.ml.param.shared.HasBlockSize)\n","        MultilayerPerceptronClassificationModel(_JavaProbabilisticClassificationModel, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        MultilayerPerceptronClassifier(_JavaProbabilisticClassifier, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    _NaiveBayesParams(pyspark.ml.base._PredictorParams, pyspark.ml.param.shared.HasWeightCol)\n","        NaiveBayes(_JavaProbabilisticClassifier, _NaiveBayesParams, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        NaiveBayesModel(_JavaProbabilisticClassificationModel, _NaiveBayesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    _OneVsRestParams(_ClassifierParams, pyspark.ml.param.shared.HasWeightCol)\n","        OneVsRest(pyspark.ml.base.Estimator, _OneVsRestParams, pyspark.ml.param.shared.HasParallelism, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable, typing.Generic)\n","        OneVsRestModel(pyspark.ml.base.Model, _OneVsRestParams, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable)\n","    _RandomForestClassifierParams(pyspark.ml.tree._RandomForestParams, pyspark.ml.tree._TreeClassifierParams)\n","        RandomForestClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        RandomForestClassifier(_JavaProbabilisticClassifier, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    pyspark.ml.param.shared.HasParallelism(pyspark.ml.param.Params)\n","        OneVsRest(pyspark.ml.base.Estimator, _OneVsRestParams, pyspark.ml.param.shared.HasParallelism, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable, typing.Generic)\n","    pyspark.ml.param.shared.HasThresholds(pyspark.ml.param.Params)\n","        NaiveBayes(_JavaProbabilisticClassifier, _NaiveBayesParams, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    pyspark.ml.param.shared.HasWeightCol(pyspark.ml.param.Params)\n","        NaiveBayes(_JavaProbabilisticClassifier, _NaiveBayesParams, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    pyspark.ml.regression._FactorizationMachinesParams(pyspark.ml.base._PredictorParams, pyspark.ml.param.shared.HasMaxIter, pyspark.ml.param.shared.HasStepSize, pyspark.ml.param.shared.HasTol, pyspark.ml.param.shared.HasSolver, pyspark.ml.param.shared.HasSeed, pyspark.ml.param.shared.HasFitIntercept, pyspark.ml.param.shared.HasRegParam, pyspark.ml.param.shared.HasWeightCol)\n","        FMClassificationModel(_JavaProbabilisticClassificationModel, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        FMClassifier(_JavaProbabilisticClassifier, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    pyspark.ml.tree._DecisionTreeModel(pyspark.ml.wrapper.JavaPredictionModel)\n","        DecisionTreeClassificationModel(pyspark.ml.tree._DecisionTreeModel, _JavaProbabilisticClassificationModel, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    pyspark.ml.tree._TreeEnsembleModel(pyspark.ml.wrapper.JavaPredictionModel)\n","        GBTClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        RandomForestClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","    pyspark.ml.util.HasTrainingSummary(typing.Generic)\n","        FMClassificationModel(_JavaProbabilisticClassificationModel, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        LinearSVCModel(_JavaClassificationModel, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        LogisticRegressionModel(_JavaProbabilisticClassificationModel, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        MultilayerPerceptronClassificationModel(_JavaProbabilisticClassificationModel, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        RandomForestClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","    pyspark.ml.util.JavaMLReadable(pyspark.ml.util.MLReadable)\n","        DecisionTreeClassificationModel(pyspark.ml.tree._DecisionTreeModel, _JavaProbabilisticClassificationModel, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        DecisionTreeClassifier(_JavaProbabilisticClassifier, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        FMClassificationModel(_JavaProbabilisticClassificationModel, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        FMClassifier(_JavaProbabilisticClassifier, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        GBTClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        GBTClassifier(_JavaProbabilisticClassifier, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LinearSVC(_JavaClassifier, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LinearSVCModel(_JavaClassificationModel, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        LogisticRegression(_JavaProbabilisticClassifier, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LogisticRegressionModel(_JavaProbabilisticClassificationModel, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        MultilayerPerceptronClassificationModel(_JavaProbabilisticClassificationModel, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        MultilayerPerceptronClassifier(_JavaProbabilisticClassifier, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        NaiveBayes(_JavaProbabilisticClassifier, _NaiveBayesParams, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        NaiveBayesModel(_JavaProbabilisticClassificationModel, _NaiveBayesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        RandomForestClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        RandomForestClassifier(_JavaProbabilisticClassifier, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    pyspark.ml.util.JavaMLWritable(pyspark.ml.util.MLWritable)\n","        DecisionTreeClassificationModel(pyspark.ml.tree._DecisionTreeModel, _JavaProbabilisticClassificationModel, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        DecisionTreeClassifier(_JavaProbabilisticClassifier, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        FMClassificationModel(_JavaProbabilisticClassificationModel, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        FMClassifier(_JavaProbabilisticClassifier, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        GBTClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        GBTClassifier(_JavaProbabilisticClassifier, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LinearSVC(_JavaClassifier, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LinearSVCModel(_JavaClassificationModel, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        LogisticRegression(_JavaProbabilisticClassifier, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        LogisticRegressionModel(_JavaProbabilisticClassificationModel, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        MultilayerPerceptronClassificationModel(_JavaProbabilisticClassificationModel, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        MultilayerPerceptronClassifier(_JavaProbabilisticClassifier, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        NaiveBayes(_JavaProbabilisticClassifier, _NaiveBayesParams, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        NaiveBayesModel(_JavaProbabilisticClassificationModel, _NaiveBayesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","        RandomForestClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","        RandomForestClassifier(_JavaProbabilisticClassifier, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","    pyspark.ml.util.MLReadable(typing.Generic)\n","        OneVsRest(pyspark.ml.base.Estimator, _OneVsRestParams, pyspark.ml.param.shared.HasParallelism, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable, typing.Generic)\n","        OneVsRestModel(pyspark.ml.base.Model, _OneVsRestParams, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable)\n","    pyspark.ml.util.MLWritable(builtins.object)\n","        OneVsRest(pyspark.ml.base.Estimator, _OneVsRestParams, pyspark.ml.param.shared.HasParallelism, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable, typing.Generic)\n","        OneVsRestModel(pyspark.ml.base.Model, _OneVsRestParams, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable)\n","    typing.Generic(builtins.object)\n","        OneVsRest(pyspark.ml.base.Estimator, _OneVsRestParams, pyspark.ml.param.shared.HasParallelism, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable, typing.Generic)\n","    \n","    class BinaryLogisticRegressionSummary(_BinaryClassificationSummary, LogisticRegressionSummary)\n","     |  BinaryLogisticRegressionSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Binary Logistic regression results for a given model.\n","     |  \n","     |  .. versionadded:: 2.0.0\n","     |  \n","     |  Method resolution order:\n","     |      BinaryLogisticRegressionSummary\n","     |      _BinaryClassificationSummary\n","     |      LogisticRegressionSummary\n","     |      _ClassificationSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from LogisticRegressionSummary:\n","     |  \n","     |  featuresCol\n","     |      Field in \"predictions\" which gives the features of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  probabilityCol\n","     |      Field in \"predictions\" which gives the probability\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class BinaryLogisticRegressionTrainingSummary(BinaryLogisticRegressionSummary, LogisticRegressionTrainingSummary)\n","     |  BinaryLogisticRegressionTrainingSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Binary Logistic regression training results for a given model.\n","     |  \n","     |  .. versionadded:: 2.0.0\n","     |  \n","     |  Method resolution order:\n","     |      BinaryLogisticRegressionTrainingSummary\n","     |      BinaryLogisticRegressionSummary\n","     |      _BinaryClassificationSummary\n","     |      LogisticRegressionTrainingSummary\n","     |      LogisticRegressionSummary\n","     |      _ClassificationSummary\n","     |      _TrainingSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from LogisticRegressionSummary:\n","     |  \n","     |  featuresCol\n","     |      Field in \"predictions\" which gives the features of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  probabilityCol\n","     |      Field in \"predictions\" which gives the probability\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _TrainingSummary:\n","     |  \n","     |  objectiveHistory\n","     |      Objective function (scaled loss + regularization) at each\n","     |      iteration. It contains one more element, the initial state,\n","     |      than number of iterations.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  totalIterations\n","     |      Number of training iterations until termination.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class BinaryRandomForestClassificationSummary(_BinaryClassificationSummary)\n","     |  BinaryRandomForestClassificationSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  BinaryRandomForestClassification results for a given model.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      BinaryRandomForestClassificationSummary\n","     |      _BinaryClassificationSummary\n","     |      _ClassificationSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class BinaryRandomForestClassificationTrainingSummary(BinaryRandomForestClassificationSummary, RandomForestClassificationTrainingSummary)\n","     |  BinaryRandomForestClassificationTrainingSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  BinaryRandomForestClassification training results for a given model.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      BinaryRandomForestClassificationTrainingSummary\n","     |      BinaryRandomForestClassificationSummary\n","     |      _BinaryClassificationSummary\n","     |      RandomForestClassificationTrainingSummary\n","     |      RandomForestClassificationSummary\n","     |      _ClassificationSummary\n","     |      _TrainingSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _TrainingSummary:\n","     |  \n","     |  objectiveHistory\n","     |      Objective function (scaled loss + regularization) at each\n","     |      iteration. It contains one more element, the initial state,\n","     |      than number of iterations.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  totalIterations\n","     |      Number of training iterations until termination.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class DecisionTreeClassificationModel(pyspark.ml.tree._DecisionTreeModel, _JavaProbabilisticClassificationModel, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  DecisionTreeClassificationModel(*args, **kwds)\n","     |  \n","     |  Model fitted by DecisionTreeClassifier.\n","     |  \n","     |  .. versionadded:: 1.4.0\n","     |  \n","     |  Method resolution order:\n","     |      DecisionTreeClassificationModel\n","     |      pyspark.ml.tree._DecisionTreeModel\n","     |      _JavaProbabilisticClassificationModel\n","     |      ProbabilisticClassificationModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      _DecisionTreeClassifierParams\n","     |      pyspark.ml.tree._DecisionTreeParams\n","     |      pyspark.ml.param.shared.HasCheckpointInterval\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.tree._TreeClassifierParams\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Readonly properties defined here:\n","     |  \n","     |  featureImportances\n","     |      Estimate of the importance of each feature.\n","     |      \n","     |      This generalizes the idea of \"Gini\" importance to other losses,\n","     |      following the explanation of Gini importance from \"Random Forests\" documentation\n","     |      by Leo Breiman and Adele Cutler, and following the implementation from scikit-learn.\n","     |      \n","     |      This feature importance is calculated as follows:\n","     |        - importance(feature j) = sum (over nodes which split on feature j) of the gain,\n","     |          where gain is scaled by the number of instances passing through node\n","     |        - Normalize importances for tree to sum to 1.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      Feature importance for single decision trees can have high variance due to\n","     |      correlated predictor variables. Consider using a :py:class:`RandomForestClassifier`\n","     |      to determine feature importance instead.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (<class 'pyspark.ml.tree._DecisionTreeModel'>, pyspar...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._DecisionTreeModel:\n","     |  \n","     |  predictLeaf(self, value: pyspark.ml.linalg.Vector) -> float\n","     |      Predict the indices of the leaves corresponding to the feature vector.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.tree._DecisionTreeModel:\n","     |  \n","     |  depth\n","     |      Return depth of the decision tree.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  numNodes\n","     |      Return number of nodes of the decision tree.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  toDebugString\n","     |      Full description of model.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaProbabilisticClassificationModel:\n","     |  \n","     |  predictProbability(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Predict the probability of each class given the features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassificationModel:\n","     |  \n","     |  setProbabilityCol(self: ~CM, value: str) -> ~CM\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: ~CM, value: List[float]) -> ~CM\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  __annotations__ = {'probabilityCol': 'Param[str]'}\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  getCacheNodeIds(self) -> bool\n","     |      Gets the value of cacheNodeIds or its default value.\n","     |  \n","     |  getLeafCol(self) -> str\n","     |      Gets the value of leafCol or its default value.\n","     |  \n","     |  getMaxBins(self) -> int\n","     |      Gets the value of maxBins or its default value.\n","     |  \n","     |  getMaxDepth(self) -> int\n","     |      Gets the value of maxDepth or its default value.\n","     |  \n","     |  getMaxMemoryInMB(self) -> int\n","     |      Gets the value of maxMemoryInMB or its default value.\n","     |  \n","     |  getMinInfoGain(self) -> float\n","     |      Gets the value of minInfoGain or its default value.\n","     |  \n","     |  getMinInstancesPerNode(self) -> int\n","     |      Gets the value of minInstancesPerNode or its default value.\n","     |  \n","     |  getMinWeightFractionPerNode(self) -> float\n","     |      Gets the value of minWeightFractionPerNode or its default value.\n","     |  \n","     |  setLeafCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`leafCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n","     |  \n","     |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n","     |  \n","     |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n","     |  \n","     |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n","     |  \n","     |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n","     |  \n","     |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n","     |  \n","     |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n","     |  \n","     |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  getCheckpointInterval(self) -> int\n","     |      Gets the value of checkpointInterval or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  getImpurity(self) -> str\n","     |      Gets the value of impurity or its default value.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  impurity = Param(parent='undefined', name='impurity', doc='...-insensi...\n","     |  \n","     |  supportedImpurities = ['entropy', 'gini']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class DecisionTreeClassifier(_JavaProbabilisticClassifier, _DecisionTreeClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  DecisionTreeClassifier(*args, **kwds)\n","     |  \n","     |  `Decision tree <http://en.wikipedia.org/wiki/Decision_tree_learning>`_\n","     |  learning algorithm for classification.\n","     |  It supports both binary and multiclass labels, as well as both continuous and categorical\n","     |  features.\n","     |  \n","     |  .. versionadded:: 1.4.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> from pyspark.ml.feature import StringIndexer\n","     |  >>> df = spark.createDataFrame([\n","     |  ...     (1.0, Vectors.dense(1.0)),\n","     |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n","     |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n","     |  >>> si_model = stringIndexer.fit(df)\n","     |  >>> td = si_model.transform(df)\n","     |  >>> dt = DecisionTreeClassifier(maxDepth=2, labelCol=\"indexed\", leafCol=\"leafId\")\n","     |  >>> model = dt.fit(td)\n","     |  >>> model.getLabelCol()\n","     |  'indexed'\n","     |  >>> model.setFeaturesCol(\"features\")\n","     |  DecisionTreeClassificationModel...\n","     |  >>> model.numNodes\n","     |  3\n","     |  >>> model.depth\n","     |  1\n","     |  >>> model.featureImportances\n","     |  SparseVector(1, {0: 1.0})\n","     |  >>> model.numFeatures\n","     |  1\n","     |  >>> model.numClasses\n","     |  2\n","     |  >>> print(model.toDebugString)\n","     |  DecisionTreeClassificationModel...depth=1, numNodes=3...\n","     |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n","     |  >>> model.predict(test0.head().features)\n","     |  0.0\n","     |  >>> model.predictRaw(test0.head().features)\n","     |  DenseVector([1.0, 0.0])\n","     |  >>> model.predictProbability(test0.head().features)\n","     |  DenseVector([1.0, 0.0])\n","     |  >>> result = model.transform(test0).head()\n","     |  >>> result.prediction\n","     |  0.0\n","     |  >>> result.probability\n","     |  DenseVector([1.0, 0.0])\n","     |  >>> result.rawPrediction\n","     |  DenseVector([1.0, 0.0])\n","     |  >>> result.leafId\n","     |  0.0\n","     |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n","     |  >>> model.transform(test1).head().prediction\n","     |  1.0\n","     |  >>> dtc_path = temp_path + \"/dtc\"\n","     |  >>> dt.save(dtc_path)\n","     |  >>> dt2 = DecisionTreeClassifier.load(dtc_path)\n","     |  >>> dt2.getMaxDepth()\n","     |  2\n","     |  >>> model_path = temp_path + \"/dtc_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = DecisionTreeClassificationModel.load(model_path)\n","     |  >>> model.featureImportances == model2.featureImportances\n","     |  True\n","     |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  >>> df3 = spark.createDataFrame([\n","     |  ...     (1.0, 0.2, Vectors.dense(1.0)),\n","     |  ...     (1.0, 0.8, Vectors.dense(1.0)),\n","     |  ...     (0.0, 1.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n","     |  >>> si3 = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n","     |  >>> si_model3 = si3.fit(df3)\n","     |  >>> td3 = si_model3.transform(df3)\n","     |  >>> dt3 = DecisionTreeClassifier(maxDepth=2, weightCol=\"weight\", labelCol=\"indexed\")\n","     |  >>> model3 = dt3.fit(td3)\n","     |  >>> print(model3.toDebugString)\n","     |  DecisionTreeClassificationModel...depth=1, numNodes=3...\n","     |  \n","     |  Method resolution order:\n","     |      DecisionTreeClassifier\n","     |      _JavaProbabilisticClassifier\n","     |      ProbabilisticClassifier\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      _DecisionTreeClassifierParams\n","     |      pyspark.ml.tree._DecisionTreeParams\n","     |      pyspark.ml.param.shared.HasCheckpointInterval\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.tree._TreeClassifierParams\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, impurity: str = 'gini', seed: Union[int, NoneType] = None, weightCol: Union[str, NoneType] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\",                  seed=None, weightCol=None, leafCol=\"\", minWeightFractionPerNode=0.0)\n","     |  \n","     |  setCacheNodeIds(self, value: bool) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`cacheNodeIds`.\n","     |  \n","     |  setCheckpointInterval(self, value: int) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`checkpointInterval`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setImpurity(self, value: str) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`impurity`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setMaxBins(self, value: int) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`maxBins`.\n","     |  \n","     |  setMaxDepth(self, value: int) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`maxDepth`.\n","     |  \n","     |  setMaxMemoryInMB(self, value: int) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`maxMemoryInMB`.\n","     |  \n","     |  setMinInfoGain(self, value: float) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`minInfoGain`.\n","     |  \n","     |  setMinInstancesPerNode(self, value: int) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`minInstancesPerNode`.\n","     |  \n","     |  setMinWeightFractionPerNode(self, value: float) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`minWeightFractionPerNode`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, impurity: str = 'gini', seed: Union[int, NoneType] = None, weightCol: Union[str, NoneType] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0) -> 'DecisionTreeClassifier'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\",                   seed=None, weightCol=None, leafCol=\"\", minWeightFractionPerNode=0.0)\n","     |      Sets params for the DecisionTreeClassifier.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setSeed(self, value: int) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`seed`.\n","     |  \n","     |  setWeightCol(self, value: str) -> 'DecisionTreeClassifier'\n","     |      Sets the value of :py:attr:`weightCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClas...e...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassifier:\n","     |  \n","     |  setProbabilityCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: 'P', value: List[float]) -> 'P'\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  getCacheNodeIds(self) -> bool\n","     |      Gets the value of cacheNodeIds or its default value.\n","     |  \n","     |  getLeafCol(self) -> str\n","     |      Gets the value of leafCol or its default value.\n","     |  \n","     |  getMaxBins(self) -> int\n","     |      Gets the value of maxBins or its default value.\n","     |  \n","     |  getMaxDepth(self) -> int\n","     |      Gets the value of maxDepth or its default value.\n","     |  \n","     |  getMaxMemoryInMB(self) -> int\n","     |      Gets the value of maxMemoryInMB or its default value.\n","     |  \n","     |  getMinInfoGain(self) -> float\n","     |      Gets the value of minInfoGain or its default value.\n","     |  \n","     |  getMinInstancesPerNode(self) -> int\n","     |      Gets the value of minInstancesPerNode or its default value.\n","     |  \n","     |  getMinWeightFractionPerNode(self) -> float\n","     |      Gets the value of minWeightFractionPerNode or its default value.\n","     |  \n","     |  setLeafCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`leafCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n","     |  \n","     |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n","     |  \n","     |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n","     |  \n","     |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n","     |  \n","     |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n","     |  \n","     |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n","     |  \n","     |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n","     |  \n","     |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  getCheckpointInterval(self) -> int\n","     |      Gets the value of checkpointInterval or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  getImpurity(self) -> str\n","     |      Gets the value of impurity or its default value.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  impurity = Param(parent='undefined', name='impurity', doc='...-insensi...\n","     |  \n","     |  supportedImpurities = ['entropy', 'gini']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class FMClassificationModel(_JavaProbabilisticClassificationModel, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","     |  FMClassificationModel(*args, **kwds)\n","     |  \n","     |  Model fitted by :class:`FMClassifier`.\n","     |  \n","     |  .. versionadded:: 3.0.0\n","     |  \n","     |  Method resolution order:\n","     |      FMClassificationModel\n","     |      _JavaProbabilisticClassificationModel\n","     |      ProbabilisticClassificationModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.regression._FactorizationMachinesParams\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasStepSize\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasSolver\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasFitIntercept\n","     |      pyspark.ml.param.shared.HasRegParam\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      pyspark.ml.util.HasTrainingSummary\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  evaluate(self, dataset: pyspark.sql.dataframe.DataFrame) -> 'FMClassificationSummary'\n","     |      Evaluates the model on a test dataset.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          Test dataset to evaluate model on.\n","     |  \n","     |  summary(self) -> 'FMClassificationTrainingSummary'\n","     |      Gets summary (accuracy/precision/recall, objective history, total iterations) of model\n","     |      trained on the training set. An exception is thrown if `trainingSummary is None`.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  factors\n","     |      Model factor term.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  intercept\n","     |      Model intercept.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  linear\n","     |      Model linear term.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaProbabilisticClassificationModel:\n","     |  \n","     |  predictProbability(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Predict the probability of each class given the features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassificationModel:\n","     |  \n","     |  setProbabilityCol(self: ~CM, value: str) -> ~CM\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: ~CM, value: List[float]) -> ~CM\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  __annotations__ = {'probabilityCol': 'Param[str]'}\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.regression._FactorizationMachinesParams:\n","     |  \n","     |  getFactorSize(self) -> int\n","     |      Gets the value of factorSize or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  getFitLinear(self) -> bool\n","     |      Gets the value of fitLinear or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  getInitStd(self) -> float\n","     |      Gets the value of initStd or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  getMiniBatchFraction(self) -> float\n","     |      Gets the value of miniBatchFraction or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.regression._FactorizationMachinesParams:\n","     |  \n","     |  factorSize = Param(parent='undefined', name='factorSize', doc... to ge...\n","     |  \n","     |  fitLinear = Param(parent='undefined', name='fitLinear', doc='whether t...\n","     |  \n","     |  initStd = Param(parent='undefined', name='initStd', doc='standard devi...\n","     |  \n","     |  miniBatchFraction = Param(parent='undefined', name='miniBatchFractio.....\n","     |  \n","     |  solver = Param(parent='undefined', name='solver', doc='Th.... Supporte...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  getStepSize(self) -> float\n","     |      Gets the value of stepSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  stepSize = Param(parent='undefined', name='stepSize', doc='...used for...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n","     |  \n","     |  getSolver(self) -> str\n","     |      Gets the value of solver or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  getFitIntercept(self) -> bool\n","     |      Gets the value of fitIntercept or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  getRegParam(self) -> float\n","     |      Gets the value of regParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.util.HasTrainingSummary:\n","     |  \n","     |  hasSummary\n","     |      Indicates whether a training summary exists for this model\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class FMClassificationSummary(_BinaryClassificationSummary)\n","     |  FMClassificationSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for FMClassifier Results for a given model.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      FMClassificationSummary\n","     |      _BinaryClassificationSummary\n","     |      _ClassificationSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class FMClassificationTrainingSummary(FMClassificationSummary, _TrainingSummary)\n","     |  FMClassificationTrainingSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for FMClassifier Training results.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      FMClassificationTrainingSummary\n","     |      FMClassificationSummary\n","     |      _BinaryClassificationSummary\n","     |      _ClassificationSummary\n","     |      _TrainingSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _TrainingSummary:\n","     |  \n","     |  objectiveHistory\n","     |      Objective function (scaled loss + regularization) at each\n","     |      iteration. It contains one more element, the initial state,\n","     |      than number of iterations.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  totalIterations\n","     |      Number of training iterations until termination.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class FMClassifier(_JavaProbabilisticClassifier, pyspark.ml.regression._FactorizationMachinesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  FMClassifier(*args, **kwds)\n","     |  \n","     |  Factorization Machines learning algorithm for classification.\n","     |  \n","     |  Solver supports:\n","     |  \n","     |  * gd (normal mini-batch gradient descent)\n","     |  * adamW (default)\n","     |  \n","     |  .. versionadded:: 3.0.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> from pyspark.ml.classification import FMClassifier\n","     |  >>> df = spark.createDataFrame([\n","     |  ...     (1.0, Vectors.dense(1.0)),\n","     |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n","     |  >>> fm = FMClassifier(factorSize=2)\n","     |  >>> fm.setSeed(11)\n","     |  FMClassifier...\n","     |  >>> model = fm.fit(df)\n","     |  >>> model.getMaxIter()\n","     |  100\n","     |  >>> test0 = spark.createDataFrame([\n","     |  ...     (Vectors.dense(-1.0),),\n","     |  ...     (Vectors.dense(0.5),),\n","     |  ...     (Vectors.dense(1.0),),\n","     |  ...     (Vectors.dense(2.0),)], [\"features\"])\n","     |  >>> model.predictRaw(test0.head().features)\n","     |  DenseVector([22.13..., -22.13...])\n","     |  >>> model.predictProbability(test0.head().features)\n","     |  DenseVector([1.0, 0.0])\n","     |  >>> model.transform(test0).select(\"features\", \"probability\").show(10, False)\n","     |  +--------+------------------------------------------+\n","     |  |features|probability                               |\n","     |  +--------+------------------------------------------+\n","     |  |[-1.0]  |[0.9999999997574736,2.425264676902229E-10]|\n","     |  |[0.5]   |[0.47627851732981163,0.5237214826701884]  |\n","     |  |[1.0]   |[5.491554426243495E-4,0.9994508445573757] |\n","     |  |[2.0]   |[2.005766663870645E-10,0.9999999997994233]|\n","     |  +--------+------------------------------------------+\n","     |  ...\n","     |  >>> model.intercept\n","     |  -7.316665276826291\n","     |  >>> model.linear\n","     |  DenseVector([14.8232])\n","     |  >>> model.factors\n","     |  DenseMatrix(1, 2, [0.0163, -0.0051], 1)\n","     |  >>> model_path = temp_path + \"/fm_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = FMClassificationModel.load(model_path)\n","     |  >>> model2.intercept\n","     |  -7.316665276826291\n","     |  >>> model2.linear\n","     |  DenseVector([14.8232])\n","     |  >>> model2.factors\n","     |  DenseMatrix(1, 2, [0.0163, -0.0051], 1)\n","     |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  \n","     |  Method resolution order:\n","     |      FMClassifier\n","     |      _JavaProbabilisticClassifier\n","     |      ProbabilisticClassifier\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.regression._FactorizationMachinesParams\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasStepSize\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasSolver\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasFitIntercept\n","     |      pyspark.ml.param.shared.HasRegParam\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', factorSize: int = 8, fitIntercept: bool = True, fitLinear: bool = True, regParam: float = 0.0, miniBatchFraction: float = 1.0, initStd: float = 0.01, maxIter: int = 100, stepSize: float = 1.0, tol: float = 1e-06, solver: str = 'adamW', thresholds: Union[List[float], NoneType] = None, seed: Union[int, NoneType] = None)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                  factorSize=8, fitIntercept=True, fitLinear=True, regParam=0.0,                  miniBatchFraction=1.0, initStd=0.01, maxIter=100, stepSize=1.0,                  tol=1e-6, solver=\"adamW\", thresholds=None, seed=None)\n","     |  \n","     |  setFactorSize(self, value: int) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`factorSize`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setFitIntercept(self, value: bool) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`fitIntercept`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setFitLinear(self, value: bool) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`fitLinear`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setInitStd(self, value: float) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`initStd`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setMaxIter(self, value: int) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`maxIter`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setMiniBatchFraction(self, value: float) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`miniBatchFraction`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', factorSize: int = 8, fitIntercept: bool = True, fitLinear: bool = True, regParam: float = 0.0, miniBatchFraction: float = 1.0, initStd: float = 0.01, maxIter: int = 100, stepSize: float = 1.0, tol: float = 1e-06, solver: str = 'adamW', thresholds: Union[List[float], NoneType] = None, seed: Union[int, NoneType] = None) -> 'FMClassifier'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                   factorSize=8, fitIntercept=True, fitLinear=True, regParam=0.0,                   miniBatchFraction=1.0, initStd=0.01, maxIter=100, stepSize=1.0,                   tol=1e-6, solver=\"adamW\", thresholds=None, seed=None)\n","     |      Sets Params for FMClassifier.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setRegParam(self, value: float) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`regParam`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setSeed(self, value: int) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`seed`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setSolver(self, value: str) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`solver`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setStepSize(self, value: float) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`stepSize`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setTol(self, value: float) -> 'FMClassifier'\n","     |      Sets the value of :py:attr:`tol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassifier:\n","     |  \n","     |  setProbabilityCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: 'P', value: List[float]) -> 'P'\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.regression._FactorizationMachinesParams:\n","     |  \n","     |  getFactorSize(self) -> int\n","     |      Gets the value of factorSize or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  getFitLinear(self) -> bool\n","     |      Gets the value of fitLinear or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  getInitStd(self) -> float\n","     |      Gets the value of initStd or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  getMiniBatchFraction(self) -> float\n","     |      Gets the value of miniBatchFraction or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.regression._FactorizationMachinesParams:\n","     |  \n","     |  factorSize = Param(parent='undefined', name='factorSize', doc... to ge...\n","     |  \n","     |  fitLinear = Param(parent='undefined', name='fitLinear', doc='whether t...\n","     |  \n","     |  initStd = Param(parent='undefined', name='initStd', doc='standard devi...\n","     |  \n","     |  miniBatchFraction = Param(parent='undefined', name='miniBatchFractio.....\n","     |  \n","     |  solver = Param(parent='undefined', name='solver', doc='Th.... Supporte...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  getStepSize(self) -> float\n","     |      Gets the value of stepSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  stepSize = Param(parent='undefined', name='stepSize', doc='...used for...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n","     |  \n","     |  getSolver(self) -> str\n","     |      Gets the value of solver or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  getFitIntercept(self) -> bool\n","     |      Gets the value of fitIntercept or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  getRegParam(self) -> float\n","     |      Gets the value of regParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class GBTClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  GBTClassificationModel(*args, **kwds)\n","     |  \n","     |  Model fitted by GBTClassifier.\n","     |  \n","     |  .. versionadded:: 1.4.0\n","     |  \n","     |  Method resolution order:\n","     |      GBTClassificationModel\n","     |      pyspark.ml.tree._TreeEnsembleModel\n","     |      _JavaProbabilisticClassificationModel\n","     |      ProbabilisticClassificationModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      _GBTClassifierParams\n","     |      pyspark.ml.tree._GBTParams\n","     |      pyspark.ml.tree._TreeEnsembleParams\n","     |      pyspark.ml.tree._DecisionTreeParams\n","     |      pyspark.ml.param.shared.HasCheckpointInterval\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasStepSize\n","     |      pyspark.ml.param.shared.HasValidationIndicatorCol\n","     |      pyspark.ml.tree._HasVarianceImpurity\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  evaluateEachIteration(self, dataset: pyspark.sql.dataframe.DataFrame) -> List[float]\n","     |      Method to compute error or loss for every iteration of gradient boosting.\n","     |      \n","     |      .. versionadded:: 2.4.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          Test dataset to evaluate model on.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  featureImportances\n","     |      Estimate of the importance of each feature.\n","     |      \n","     |      Each feature's importance is the average of its importance across all trees in the ensemble\n","     |      The importance vector is normalized to sum to 1. This method is suggested by Hastie et al.\n","     |      (Hastie, Tibshirani, Friedman. \"The Elements of Statistical Learning, 2nd Edition.\" 2001.)\n","     |      and follows the implementation from scikit-learn.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      DecisionTreeClassificationModel.featureImportances\n","     |  \n","     |  trees\n","     |      Trees in this ensemble. Warning: These have null parent Estimators.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (<class 'pyspark.ml.tree._TreeEnsembleModel'>, pyspar...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeEnsembleModel:\n","     |  \n","     |  predictLeaf(self, value: pyspark.ml.linalg.Vector) -> float\n","     |      Predict the indices of the leaves corresponding to the feature vector.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.tree._TreeEnsembleModel:\n","     |  \n","     |  getNumTrees\n","     |      Number of trees in ensemble.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  toDebugString\n","     |      Full description of model.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  totalNumNodes\n","     |      Total number of nodes, summed over all trees in the ensemble.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  treeWeights\n","     |      Return the weights for each tree\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaProbabilisticClassificationModel:\n","     |  \n","     |  predictProbability(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Predict the probability of each class given the features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassificationModel:\n","     |  \n","     |  setProbabilityCol(self: ~CM, value: str) -> ~CM\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: ~CM, value: List[float]) -> ~CM\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  __annotations__ = {'probabilityCol': 'Param[str]'}\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _GBTClassifierParams:\n","     |  \n","     |  getLossType(self) -> str\n","     |      Gets the value of lossType or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _GBTClassifierParams:\n","     |  \n","     |  lossType = Param(parent='undefined', name='lossType', doc='...(case-in...\n","     |  \n","     |  supportedLossTypes = ['logistic']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._GBTParams:\n","     |  \n","     |  getValidationTol(self) -> float\n","     |      Gets the value of validationTol or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._GBTParams:\n","     |  \n","     |  stepSize = Param(parent='undefined', name='stepSize', doc='...r shrink...\n","     |  \n","     |  validationTol = Param(parent='undefined', name='validationTol', ...is ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  getFeatureSubsetStrategy(self) -> str\n","     |      Gets the value of featureSubsetStrategy or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  getSubsamplingRate(self) -> float\n","     |      Gets the value of subsamplingRate or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n","     |  \n","     |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n","     |  \n","     |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  getCacheNodeIds(self) -> bool\n","     |      Gets the value of cacheNodeIds or its default value.\n","     |  \n","     |  getLeafCol(self) -> str\n","     |      Gets the value of leafCol or its default value.\n","     |  \n","     |  getMaxBins(self) -> int\n","     |      Gets the value of maxBins or its default value.\n","     |  \n","     |  getMaxDepth(self) -> int\n","     |      Gets the value of maxDepth or its default value.\n","     |  \n","     |  getMaxMemoryInMB(self) -> int\n","     |      Gets the value of maxMemoryInMB or its default value.\n","     |  \n","     |  getMinInfoGain(self) -> float\n","     |      Gets the value of minInfoGain or its default value.\n","     |  \n","     |  getMinInstancesPerNode(self) -> int\n","     |      Gets the value of minInstancesPerNode or its default value.\n","     |  \n","     |  getMinWeightFractionPerNode(self) -> float\n","     |      Gets the value of minWeightFractionPerNode or its default value.\n","     |  \n","     |  setLeafCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`leafCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n","     |  \n","     |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n","     |  \n","     |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n","     |  \n","     |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n","     |  \n","     |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n","     |  \n","     |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n","     |  \n","     |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n","     |  \n","     |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  getCheckpointInterval(self) -> int\n","     |      Gets the value of checkpointInterval or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  getStepSize(self) -> float\n","     |      Gets the value of stepSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasValidationIndicatorCol:\n","     |  \n","     |  getValidationIndicatorCol(self) -> str\n","     |      Gets the value of validationIndicatorCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasValidationIndicatorCol:\n","     |  \n","     |  validationIndicatorCol = Param(parent='undefined', name='validationInd...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._HasVarianceImpurity:\n","     |  \n","     |  getImpurity(self) -> str\n","     |      Gets the value of impurity or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._HasVarianceImpurity:\n","     |  \n","     |  impurity = Param(parent='undefined', name='impurity', doc='...(case-in...\n","     |  \n","     |  supportedImpurities = ['variance']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class GBTClassifier(_JavaProbabilisticClassifier, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  GBTClassifier(*args, **kwds)\n","     |  \n","     |  `Gradient-Boosted Trees (GBTs) <http://en.wikipedia.org/wiki/Gradient_boosting>`_\n","     |  learning algorithm for classification.\n","     |  It supports binary labels, as well as both continuous and categorical features.\n","     |  \n","     |  .. versionadded:: 1.4.0\n","     |  \n","     |  Notes\n","     |  -----\n","     |  Multiclass labels are not currently supported.\n","     |  \n","     |  The implementation is based upon: J.H. Friedman. \"Stochastic Gradient Boosting.\" 1999.\n","     |  \n","     |  Gradient Boosting vs. TreeBoost:\n","     |  \n","     |  - This implementation is for Stochastic Gradient Boosting, not for TreeBoost.\n","     |  - Both algorithms learn tree ensembles by minimizing loss functions.\n","     |  - TreeBoost (Friedman, 1999) additionally modifies the outputs at tree leaf nodes\n","     |    based on the loss function, whereas the original gradient boosting method does not.\n","     |  - We expect to implement TreeBoost in the future:\n","     |    `SPARK-4240 <https://issues.apache.org/jira/browse/SPARK-4240>`_\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from numpy import allclose\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> from pyspark.ml.feature import StringIndexer\n","     |  >>> df = spark.createDataFrame([\n","     |  ...     (1.0, Vectors.dense(1.0)),\n","     |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n","     |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n","     |  >>> si_model = stringIndexer.fit(df)\n","     |  >>> td = si_model.transform(df)\n","     |  >>> gbt = GBTClassifier(maxIter=5, maxDepth=2, labelCol=\"indexed\", seed=42,\n","     |  ...     leafCol=\"leafId\")\n","     |  >>> gbt.setMaxIter(5)\n","     |  GBTClassifier...\n","     |  >>> gbt.setMinWeightFractionPerNode(0.049)\n","     |  GBTClassifier...\n","     |  >>> gbt.getMaxIter()\n","     |  5\n","     |  >>> gbt.getFeatureSubsetStrategy()\n","     |  'all'\n","     |  >>> model = gbt.fit(td)\n","     |  >>> model.getLabelCol()\n","     |  'indexed'\n","     |  >>> model.setFeaturesCol(\"features\")\n","     |  GBTClassificationModel...\n","     |  >>> model.setThresholds([0.3, 0.7])\n","     |  GBTClassificationModel...\n","     |  >>> model.getThresholds()\n","     |  [0.3, 0.7]\n","     |  >>> model.featureImportances\n","     |  SparseVector(1, {0: 1.0})\n","     |  >>> allclose(model.treeWeights, [1.0, 0.1, 0.1, 0.1, 0.1])\n","     |  True\n","     |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n","     |  >>> model.predict(test0.head().features)\n","     |  0.0\n","     |  >>> model.predictRaw(test0.head().features)\n","     |  DenseVector([1.1697, -1.1697])\n","     |  >>> model.predictProbability(test0.head().features)\n","     |  DenseVector([0.9121, 0.0879])\n","     |  >>> result = model.transform(test0).head()\n","     |  >>> result.prediction\n","     |  0.0\n","     |  >>> result.leafId\n","     |  DenseVector([0.0, 0.0, 0.0, 0.0, 0.0])\n","     |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n","     |  >>> model.transform(test1).head().prediction\n","     |  1.0\n","     |  >>> model.totalNumNodes\n","     |  15\n","     |  >>> print(model.toDebugString)\n","     |  GBTClassificationModel...numTrees=5...\n","     |  >>> gbtc_path = temp_path + \"gbtc\"\n","     |  >>> gbt.save(gbtc_path)\n","     |  >>> gbt2 = GBTClassifier.load(gbtc_path)\n","     |  >>> gbt2.getMaxDepth()\n","     |  2\n","     |  >>> model_path = temp_path + \"gbtc_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = GBTClassificationModel.load(model_path)\n","     |  >>> model.featureImportances == model2.featureImportances\n","     |  True\n","     |  >>> model.treeWeights == model2.treeWeights\n","     |  True\n","     |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  >>> model.trees\n","     |  [DecisionTreeRegressionModel...depth=..., DecisionTreeRegressionModel...]\n","     |  >>> validation = spark.createDataFrame([(0.0, Vectors.dense(-1.0),)],\n","     |  ...              [\"indexed\", \"features\"])\n","     |  >>> model.evaluateEachIteration(validation)\n","     |  [0.25..., 0.23..., 0.21..., 0.19..., 0.18...]\n","     |  >>> model.numClasses\n","     |  2\n","     |  >>> gbt = gbt.setValidationIndicatorCol(\"validationIndicator\")\n","     |  >>> gbt.getValidationIndicatorCol()\n","     |  'validationIndicator'\n","     |  >>> gbt.getValidationTol()\n","     |  0.01\n","     |  \n","     |  Method resolution order:\n","     |      GBTClassifier\n","     |      _JavaProbabilisticClassifier\n","     |      ProbabilisticClassifier\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      _GBTClassifierParams\n","     |      pyspark.ml.tree._GBTParams\n","     |      pyspark.ml.tree._TreeEnsembleParams\n","     |      pyspark.ml.tree._DecisionTreeParams\n","     |      pyspark.ml.param.shared.HasCheckpointInterval\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasStepSize\n","     |      pyspark.ml.param.shared.HasValidationIndicatorCol\n","     |      pyspark.ml.tree._HasVarianceImpurity\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, lossType: str = 'logistic', maxIter: int = 20, stepSize: float = 0.1, seed: Union[int, NoneType] = None, subsamplingRate: float = 1.0, impurity: str = 'variance', featureSubsetStrategy: str = 'all', validationTol: float = 0.01, validationIndicatorCol: Union[str, NoneType] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0, weightCol: Union[str, NoneType] = None)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                  lossType=\"logistic\", maxIter=20, stepSize=0.1, seed=None, subsamplingRate=1.0,                  impurity=\"variance\", featureSubsetStrategy=\"all\", validationTol=0.01,                  validationIndicatorCol=None, leafCol=\"\", minWeightFractionPerNode=0.0,                  weightCol=None)\n","     |  \n","     |  setCacheNodeIds(self, value: bool) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`cacheNodeIds`.\n","     |  \n","     |  setCheckpointInterval(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`checkpointInterval`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setFeatureSubsetStrategy(self, value: str) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`featureSubsetStrategy`.\n","     |      \n","     |      .. versionadded:: 2.4.0\n","     |  \n","     |  setImpurity(self, value: str) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`impurity`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setLossType(self, value: str) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`lossType`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setMaxBins(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`maxBins`.\n","     |  \n","     |  setMaxDepth(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`maxDepth`.\n","     |  \n","     |  setMaxIter(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`maxIter`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setMaxMemoryInMB(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`maxMemoryInMB`.\n","     |  \n","     |  setMinInfoGain(self, value: float) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`minInfoGain`.\n","     |  \n","     |  setMinInstancesPerNode(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`minInstancesPerNode`.\n","     |  \n","     |  setMinWeightFractionPerNode(self, value: float) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`minWeightFractionPerNode`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, lossType: str = 'logistic', maxIter: int = 20, stepSize: float = 0.1, seed: Union[int, NoneType] = None, subsamplingRate: float = 1.0, impurity: str = 'variance', featureSubsetStrategy: str = 'all', validationTol: float = 0.01, validationIndicatorCol: Union[str, NoneType] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0, weightCol: Union[str, NoneType] = None) -> 'GBTClassifier'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                   lossType=\"logistic\", maxIter=20, stepSize=0.1, seed=None, subsamplingRate=1.0,                   impurity=\"variance\", featureSubsetStrategy=\"all\", validationTol=0.01,                   validationIndicatorCol=None, leafCol=\"\", minWeightFractionPerNode=0.0,                   weightCol=None)\n","     |      Sets params for Gradient Boosted Tree Classification.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setSeed(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`seed`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setStepSize(self, value: int) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`stepSize`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setSubsamplingRate(self, value: float) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`subsamplingRate`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setValidationIndicatorCol(self, value: str) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`validationIndicatorCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setWeightCol(self, value: str) -> 'GBTClassifier'\n","     |      Sets the value of :py:attr:`weightCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassifier:\n","     |  \n","     |  setProbabilityCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: 'P', value: List[float]) -> 'P'\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _GBTClassifierParams:\n","     |  \n","     |  getLossType(self) -> str\n","     |      Gets the value of lossType or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _GBTClassifierParams:\n","     |  \n","     |  lossType = Param(parent='undefined', name='lossType', doc='...(case-in...\n","     |  \n","     |  supportedLossTypes = ['logistic']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._GBTParams:\n","     |  \n","     |  getValidationTol(self) -> float\n","     |      Gets the value of validationTol or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._GBTParams:\n","     |  \n","     |  stepSize = Param(parent='undefined', name='stepSize', doc='...r shrink...\n","     |  \n","     |  validationTol = Param(parent='undefined', name='validationTol', ...is ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  getFeatureSubsetStrategy(self) -> str\n","     |      Gets the value of featureSubsetStrategy or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  getSubsamplingRate(self) -> float\n","     |      Gets the value of subsamplingRate or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n","     |  \n","     |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n","     |  \n","     |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  getCacheNodeIds(self) -> bool\n","     |      Gets the value of cacheNodeIds or its default value.\n","     |  \n","     |  getLeafCol(self) -> str\n","     |      Gets the value of leafCol or its default value.\n","     |  \n","     |  getMaxBins(self) -> int\n","     |      Gets the value of maxBins or its default value.\n","     |  \n","     |  getMaxDepth(self) -> int\n","     |      Gets the value of maxDepth or its default value.\n","     |  \n","     |  getMaxMemoryInMB(self) -> int\n","     |      Gets the value of maxMemoryInMB or its default value.\n","     |  \n","     |  getMinInfoGain(self) -> float\n","     |      Gets the value of minInfoGain or its default value.\n","     |  \n","     |  getMinInstancesPerNode(self) -> int\n","     |      Gets the value of minInstancesPerNode or its default value.\n","     |  \n","     |  getMinWeightFractionPerNode(self) -> float\n","     |      Gets the value of minWeightFractionPerNode or its default value.\n","     |  \n","     |  setLeafCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`leafCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n","     |  \n","     |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n","     |  \n","     |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n","     |  \n","     |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n","     |  \n","     |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n","     |  \n","     |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n","     |  \n","     |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n","     |  \n","     |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  getCheckpointInterval(self) -> int\n","     |      Gets the value of checkpointInterval or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  getStepSize(self) -> float\n","     |      Gets the value of stepSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasValidationIndicatorCol:\n","     |  \n","     |  getValidationIndicatorCol(self) -> str\n","     |      Gets the value of validationIndicatorCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasValidationIndicatorCol:\n","     |  \n","     |  validationIndicatorCol = Param(parent='undefined', name='validationInd...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._HasVarianceImpurity:\n","     |  \n","     |  getImpurity(self) -> str\n","     |      Gets the value of impurity or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._HasVarianceImpurity:\n","     |  \n","     |  impurity = Param(parent='undefined', name='impurity', doc='...(case-in...\n","     |  \n","     |  supportedImpurities = ['variance']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class LinearSVC(_JavaClassifier, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  LinearSVC(*args, **kwds)\n","     |  \n","     |  This binary classifier optimizes the Hinge Loss using the OWLQN optimizer.\n","     |  Only supports L2 regularization currently.\n","     |  \n","     |  .. versionadded:: 2.2.0\n","     |  \n","     |  Notes\n","     |  -----\n","     |  `Linear SVM Classifier <https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM>`_\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.sql import Row\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> df = sc.parallelize([\n","     |  ...     Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n","     |  ...     Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n","     |  >>> svm = LinearSVC()\n","     |  >>> svm.getMaxIter()\n","     |  100\n","     |  >>> svm.setMaxIter(5)\n","     |  LinearSVC...\n","     |  >>> svm.getMaxIter()\n","     |  5\n","     |  >>> svm.getRegParam()\n","     |  0.0\n","     |  >>> svm.setRegParam(0.01)\n","     |  LinearSVC...\n","     |  >>> svm.getRegParam()\n","     |  0.01\n","     |  >>> model = svm.fit(df)\n","     |  >>> model.setPredictionCol(\"newPrediction\")\n","     |  LinearSVCModel...\n","     |  >>> model.getPredictionCol()\n","     |  'newPrediction'\n","     |  >>> model.setThreshold(0.5)\n","     |  LinearSVCModel...\n","     |  >>> model.getThreshold()\n","     |  0.5\n","     |  >>> model.getMaxBlockSizeInMB()\n","     |  0.0\n","     |  >>> model.coefficients\n","     |  DenseVector([0.0, -1.0319, -0.5159])\n","     |  >>> model.intercept\n","     |  2.579645978780695\n","     |  >>> model.numClasses\n","     |  2\n","     |  >>> model.numFeatures\n","     |  3\n","     |  >>> test0 = sc.parallelize([Row(features=Vectors.dense(-1.0, -1.0, -1.0))]).toDF()\n","     |  >>> model.predict(test0.head().features)\n","     |  1.0\n","     |  >>> model.predictRaw(test0.head().features)\n","     |  DenseVector([-4.1274, 4.1274])\n","     |  >>> result = model.transform(test0).head()\n","     |  >>> result.newPrediction\n","     |  1.0\n","     |  >>> result.rawPrediction\n","     |  DenseVector([-4.1274, 4.1274])\n","     |  >>> svm_path = temp_path + \"/svm\"\n","     |  >>> svm.save(svm_path)\n","     |  >>> svm2 = LinearSVC.load(svm_path)\n","     |  >>> svm2.getMaxIter()\n","     |  5\n","     |  >>> model_path = temp_path + \"/svm_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = LinearSVCModel.load(model_path)\n","     |  >>> model.coefficients[0] == model2.coefficients[0]\n","     |  True\n","     |  >>> model.intercept == model2.intercept\n","     |  True\n","     |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  \n","     |  Method resolution order:\n","     |      LinearSVC\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _LinearSVCParams\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasRegParam\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasFitIntercept\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasStandardization\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.shared.HasAggregationDepth\n","     |      pyspark.ml.param.shared.HasThreshold\n","     |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, tol: float = 1e-06, rawPredictionCol: str = 'rawPrediction', fitIntercept: bool = True, standardization: bool = True, threshold: float = 0.0, weightCol: Union[str, NoneType] = None, aggregationDepth: int = 2, maxBlockSizeInMB: float = 0.0)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, tol=1e-6, rawPredictionCol=\"rawPrediction\",                  fitIntercept=True, standardization=True, threshold=0.0, weightCol=None,                  aggregationDepth=2, maxBlockSizeInMB=0.0):\n","     |  \n","     |  setAggregationDepth(self, value: int) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`aggregationDepth`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setFitIntercept(self, value: bool) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`fitIntercept`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setMaxBlockSizeInMB(self, value: float) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`maxBlockSizeInMB`.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  setMaxIter(self, value: int) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`maxIter`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, tol: float = 1e-06, rawPredictionCol: str = 'rawPrediction', fitIntercept: bool = True, standardization: bool = True, threshold: float = 0.0, weightCol: Union[str, NoneType] = None, aggregationDepth: int = 2, maxBlockSizeInMB: float = 0.0) -> 'LinearSVC'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxIter=100, regParam=0.0, tol=1e-6, rawPredictionCol=\"rawPrediction\",                   fitIntercept=True, standardization=True, threshold=0.0, weightCol=None,                   aggregationDepth=2, maxBlockSizeInMB=0.0):\n","     |      Sets params for Linear SVM Classifier.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setRegParam(self, value: float) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`regParam`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setStandardization(self, value: bool) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`standardization`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setThreshold(self, value: float) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`threshold`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setTol(self, value: float) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`tol`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  setWeightCol(self, value: str) -> 'LinearSVC'\n","     |      Sets the value of :py:attr:`weightCol`.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaClassifier[ForwardRef...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _LinearSVCParams:\n","     |  \n","     |  threshold = Param(parent='undefined', name='threshold', doc=...ons 0.0...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  getRegParam(self) -> float\n","     |      Gets the value of regParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  getFitIntercept(self) -> bool\n","     |      Gets the value of fitIntercept or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  getStandardization(self) -> bool\n","     |      Gets the value of standardization or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  standardization = Param(parent='undefined', name='standardization'...t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  getAggregationDepth(self) -> int\n","     |      Gets the value of aggregationDepth or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThreshold:\n","     |  \n","     |  getThreshold(self) -> float\n","     |      Gets the value of threshold or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  getMaxBlockSizeInMB(self) -> float\n","     |      Gets the value of maxBlockSizeInMB or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class LinearSVCModel(_JavaClassificationModel, _LinearSVCParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","     |  LinearSVCModel(*args, **kwds)\n","     |  \n","     |  Model fitted by LinearSVC.\n","     |  \n","     |  .. versionadded:: 2.2.0\n","     |  \n","     |  Method resolution order:\n","     |      LinearSVCModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _LinearSVCParams\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasRegParam\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasFitIntercept\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasStandardization\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.shared.HasAggregationDepth\n","     |      pyspark.ml.param.shared.HasThreshold\n","     |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      pyspark.ml.util.HasTrainingSummary\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  evaluate(self, dataset: pyspark.sql.dataframe.DataFrame) -> 'LinearSVCSummary'\n","     |      Evaluates the model on a test dataset.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          Test dataset to evaluate model on.\n","     |  \n","     |  setThreshold(self, value: float) -> 'LinearSVCModel'\n","     |      Sets the value of :py:attr:`threshold`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  summary(self) -> 'LinearSVCTrainingSummary'\n","     |      Gets summary (accuracy/precision/recall, objective history, total iterations) of model\n","     |      trained on the training set. An exception is thrown if `trainingSummary is None`.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  coefficients\n","     |      Model coefficients of Linear SVM Classifier.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  intercept\n","     |      Model intercept of Linear SVM Classifier.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaClassificationModel[p...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _LinearSVCParams:\n","     |  \n","     |  __annotations__ = {'threshold': pyspark.ml.param.Param[float]}\n","     |  \n","     |  threshold = Param(parent='undefined', name='threshold', doc=...ons 0.0...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  getRegParam(self) -> float\n","     |      Gets the value of regParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  getFitIntercept(self) -> bool\n","     |      Gets the value of fitIntercept or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  getStandardization(self) -> bool\n","     |      Gets the value of standardization or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  standardization = Param(parent='undefined', name='standardization'...t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  getAggregationDepth(self) -> int\n","     |      Gets the value of aggregationDepth or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThreshold:\n","     |  \n","     |  getThreshold(self) -> float\n","     |      Gets the value of threshold or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  getMaxBlockSizeInMB(self) -> float\n","     |      Gets the value of maxBlockSizeInMB or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.util.HasTrainingSummary:\n","     |  \n","     |  hasSummary\n","     |      Indicates whether a training summary exists for this model\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class LinearSVCSummary(_BinaryClassificationSummary)\n","     |  LinearSVCSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for LinearSVC Results for a given model.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      LinearSVCSummary\n","     |      _BinaryClassificationSummary\n","     |      _ClassificationSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class LinearSVCTrainingSummary(LinearSVCSummary, _TrainingSummary)\n","     |  LinearSVCTrainingSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for LinearSVC Training results.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      LinearSVCTrainingSummary\n","     |      LinearSVCSummary\n","     |      _BinaryClassificationSummary\n","     |      _ClassificationSummary\n","     |      _TrainingSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from _BinaryClassificationSummary:\n","     |  \n","     |  areaUnderROC\n","     |      Computes the area under the receiver operating characteristic\n","     |      (ROC) curve.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  fMeasureByThreshold\n","     |      Returns a dataframe with two fields (threshold, F-Measure) curve\n","     |      with beta = 1.0.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  pr\n","     |      Returns the precision-recall curve, which is a Dataframe\n","     |      containing two fields recall, precision with (0.0, 1.0) prepended\n","     |      to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  precisionByThreshold\n","     |      Returns a dataframe with two fields (threshold, precision) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByThreshold\n","     |      Returns a dataframe with two fields (threshold, recall) curve.\n","     |      Every possible probability obtained in transforming the dataset\n","     |      are used as thresholds used in calculating the recall.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  roc\n","     |      Returns the receiver operating characteristic (ROC) curve,\n","     |      which is a Dataframe having two fields (FPR, TPR) with\n","     |      (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      `Wikipedia reference <http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n","     |  \n","     |  scoreCol\n","     |      Field in \"predictions\" which gives the probability or raw prediction\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _TrainingSummary:\n","     |  \n","     |  objectiveHistory\n","     |      Objective function (scaled loss + regularization) at each\n","     |      iteration. It contains one more element, the initial state,\n","     |      than number of iterations.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  totalIterations\n","     |      Number of training iterations until termination.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class LogisticRegression(_JavaProbabilisticClassifier, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  LogisticRegression(*args, **kwds)\n","     |  \n","     |  Logistic regression.\n","     |  This class supports multinomial logistic (softmax) and binomial logistic regression.\n","     |  \n","     |  .. versionadded:: 1.3.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.sql import Row\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> bdf = sc.parallelize([\n","     |  ...     Row(label=1.0, weight=1.0, features=Vectors.dense(0.0, 5.0)),\n","     |  ...     Row(label=0.0, weight=2.0, features=Vectors.dense(1.0, 2.0)),\n","     |  ...     Row(label=1.0, weight=3.0, features=Vectors.dense(2.0, 1.0)),\n","     |  ...     Row(label=0.0, weight=4.0, features=Vectors.dense(3.0, 3.0))]).toDF()\n","     |  >>> blor = LogisticRegression(weightCol=\"weight\")\n","     |  >>> blor.getRegParam()\n","     |  0.0\n","     |  >>> blor.setRegParam(0.01)\n","     |  LogisticRegression...\n","     |  >>> blor.getRegParam()\n","     |  0.01\n","     |  >>> blor.setMaxIter(10)\n","     |  LogisticRegression...\n","     |  >>> blor.getMaxIter()\n","     |  10\n","     |  >>> blor.clear(blor.maxIter)\n","     |  >>> blorModel = blor.fit(bdf)\n","     |  >>> blorModel.setFeaturesCol(\"features\")\n","     |  LogisticRegressionModel...\n","     |  >>> blorModel.setProbabilityCol(\"newProbability\")\n","     |  LogisticRegressionModel...\n","     |  >>> blorModel.getProbabilityCol()\n","     |  'newProbability'\n","     |  >>> blorModel.getMaxBlockSizeInMB()\n","     |  0.0\n","     |  >>> blorModel.setThreshold(0.1)\n","     |  LogisticRegressionModel...\n","     |  >>> blorModel.getThreshold()\n","     |  0.1\n","     |  >>> blorModel.coefficients\n","     |  DenseVector([-1.080..., -0.646...])\n","     |  >>> blorModel.intercept\n","     |  3.112...\n","     |  >>> blorModel.evaluate(bdf).accuracy == blorModel.summary.accuracy\n","     |  True\n","     |  >>> data_path = \"data/mllib/sample_multiclass_classification_data.txt\"\n","     |  >>> mdf = spark.read.format(\"libsvm\").load(data_path)\n","     |  >>> mlor = LogisticRegression(regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n","     |  >>> mlorModel = mlor.fit(mdf)\n","     |  >>> mlorModel.coefficientMatrix\n","     |  SparseMatrix(3, 4, [0, 1, 2, 3], [3, 2, 1], [1.87..., -2.75..., -0.50...], 1)\n","     |  >>> mlorModel.interceptVector\n","     |  DenseVector([0.04..., -0.42..., 0.37...])\n","     |  >>> test0 = sc.parallelize([Row(features=Vectors.dense(-1.0, 1.0))]).toDF()\n","     |  >>> blorModel.predict(test0.head().features)\n","     |  1.0\n","     |  >>> blorModel.predictRaw(test0.head().features)\n","     |  DenseVector([-3.54..., 3.54...])\n","     |  >>> blorModel.predictProbability(test0.head().features)\n","     |  DenseVector([0.028, 0.972])\n","     |  >>> result = blorModel.transform(test0).head()\n","     |  >>> result.prediction\n","     |  1.0\n","     |  >>> result.newProbability\n","     |  DenseVector([0.02..., 0.97...])\n","     |  >>> result.rawPrediction\n","     |  DenseVector([-3.54..., 3.54...])\n","     |  >>> test1 = sc.parallelize([Row(features=Vectors.sparse(2, [0], [1.0]))]).toDF()\n","     |  >>> blorModel.transform(test1).head().prediction\n","     |  1.0\n","     |  >>> blor.setParams(\"vector\")\n","     |  Traceback (most recent call last):\n","     |      ...\n","     |  TypeError: Method setParams forces keyword arguments.\n","     |  >>> lr_path = temp_path + \"/lr\"\n","     |  >>> blor.save(lr_path)\n","     |  >>> lr2 = LogisticRegression.load(lr_path)\n","     |  >>> lr2.getRegParam()\n","     |  0.01\n","     |  >>> model_path = temp_path + \"/lr_model\"\n","     |  >>> blorModel.save(model_path)\n","     |  >>> model2 = LogisticRegressionModel.load(model_path)\n","     |  >>> blorModel.coefficients[0] == model2.coefficients[0]\n","     |  True\n","     |  >>> blorModel.intercept == model2.intercept\n","     |  True\n","     |  >>> model2\n","     |  LogisticRegressionModel: uid=..., numClasses=2, numFeatures=2\n","     |  >>> blorModel.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  \n","     |  Method resolution order:\n","     |      LogisticRegression\n","     |      _JavaProbabilisticClassifier\n","     |      ProbabilisticClassifier\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _LogisticRegressionParams\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasRegParam\n","     |      pyspark.ml.param.shared.HasElasticNetParam\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasFitIntercept\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasStandardization\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.shared.HasAggregationDepth\n","     |      pyspark.ml.param.shared.HasThreshold\n","     |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, threshold: float = 0.5, thresholds: Union[List[float], NoneType] = None, probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', standardization: bool = True, weightCol: Union[str, NoneType] = None, aggregationDepth: int = 2, family: str = 'auto', lowerBoundsOnCoefficients: Union[pyspark.ml.linalg.Matrix, NoneType] = None, upperBoundsOnCoefficients: Union[pyspark.ml.linalg.Matrix, NoneType] = None, lowerBoundsOnIntercepts: Union[pyspark.ml.linalg.Vector, NoneType] = None, upperBoundsOnIntercepts: Union[pyspark.ml.linalg.Vector, NoneType] = None, maxBlockSizeInMB: float = 0.0)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  threshold=0.5, thresholds=None, probabilityCol=\"probability\",                  rawPredictionCol=\"rawPrediction\", standardization=True, weightCol=None,                  aggregationDepth=2, family=\"auto\",                  lowerBoundsOnCoefficients=None, upperBoundsOnCoefficients=None,                  lowerBoundsOnIntercepts=None, upperBoundsOnIntercepts=None,                  maxBlockSizeInMB=0.0):\n","     |      If the threshold and thresholds Params are both set, they must be equivalent.\n","     |  \n","     |  setAggregationDepth(self, value: int) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`aggregationDepth`.\n","     |  \n","     |  setElasticNetParam(self, value: float) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`elasticNetParam`.\n","     |  \n","     |  setFamily(self, value: str) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`family`.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  setFitIntercept(self, value: bool) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`fitIntercept`.\n","     |  \n","     |  setLowerBoundsOnCoefficients(self, value: pyspark.ml.linalg.Matrix) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`lowerBoundsOnCoefficients`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  setLowerBoundsOnIntercepts(self, value: pyspark.ml.linalg.Vector) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`lowerBoundsOnIntercepts`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  setMaxBlockSizeInMB(self, value: float) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`maxBlockSizeInMB`.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  setMaxIter(self, value: int) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`maxIter`.\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, threshold: float = 0.5, thresholds: Union[List[float], NoneType] = None, probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', standardization: bool = True, weightCol: Union[str, NoneType] = None, aggregationDepth: int = 2, family: str = 'auto', lowerBoundsOnCoefficients: Union[pyspark.ml.linalg.Matrix, NoneType] = None, upperBoundsOnCoefficients: Union[pyspark.ml.linalg.Matrix, NoneType] = None, lowerBoundsOnIntercepts: Union[pyspark.ml.linalg.Vector, NoneType] = None, upperBoundsOnIntercepts: Union[pyspark.ml.linalg.Vector, NoneType] = None, maxBlockSizeInMB: float = 0.0) -> 'LogisticRegression'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                   threshold=0.5, thresholds=None, probabilityCol=\"probability\",                   rawPredictionCol=\"rawPrediction\", standardization=True, weightCol=None,                   aggregationDepth=2, family=\"auto\",                   lowerBoundsOnCoefficients=None, upperBoundsOnCoefficients=None,                   lowerBoundsOnIntercepts=None, upperBoundsOnIntercepts=None,                   maxBlockSizeInMB=0.0):\n","     |      Sets params for logistic regression.\n","     |      If the threshold and thresholds Params are both set, they must be equivalent.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |  \n","     |  setRegParam(self, value: float) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`regParam`.\n","     |  \n","     |  setStandardization(self, value: bool) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`standardization`.\n","     |  \n","     |  setTol(self, value: float) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`tol`.\n","     |  \n","     |  setUpperBoundsOnCoefficients(self, value: pyspark.ml.linalg.Matrix) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`upperBoundsOnCoefficients`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  setUpperBoundsOnIntercepts(self, value: pyspark.ml.linalg.Vector) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`upperBoundsOnIntercepts`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  setWeightCol(self, value: str) -> 'LogisticRegression'\n","     |      Sets the value of :py:attr:`weightCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassifier:\n","     |  \n","     |  setProbabilityCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: 'P', value: List[float]) -> 'P'\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _LogisticRegressionParams:\n","     |  \n","     |  getFamily(self) -> str\n","     |      Gets the value of :py:attr:`family` or its default value.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  getLowerBoundsOnCoefficients(self) -> pyspark.ml.linalg.Matrix\n","     |      Gets the value of :py:attr:`lowerBoundsOnCoefficients`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  getLowerBoundsOnIntercepts(self) -> pyspark.ml.linalg.Vector\n","     |      Gets the value of :py:attr:`lowerBoundsOnIntercepts`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  getThreshold(self) -> float\n","     |      Get threshold for binary classification.\n","     |      \n","     |      If :py:attr:`thresholds` is set with length 2 (i.e., binary classification),\n","     |      this returns the equivalent threshold:\n","     |      :math:`\\frac{1}{1 + \\frac{thresholds(0)}{thresholds(1)}}`.\n","     |      Otherwise, returns :py:attr:`threshold` if set or its default value if unset.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      If :py:attr:`thresholds` is set, return its value.\n","     |      Otherwise, if :py:attr:`threshold` is set, return the equivalent thresholds for binary\n","     |      classification: (1-threshold, threshold).\n","     |      If neither are set, throw an error.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  getUpperBoundsOnCoefficients(self) -> pyspark.ml.linalg.Matrix\n","     |      Gets the value of :py:attr:`upperBoundsOnCoefficients`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  getUpperBoundsOnIntercepts(self) -> pyspark.ml.linalg.Vector\n","     |      Gets the value of :py:attr:`upperBoundsOnIntercepts`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  setThreshold(self: 'P', value: float) -> 'P'\n","     |      Sets the value of :py:attr:`threshold`.\n","     |      Clears value of :py:attr:`thresholds` if it has been set.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _LogisticRegressionParams:\n","     |  \n","     |  family = Param(parent='undefined', name='family', doc='Th... Supported...\n","     |  \n","     |  lowerBoundsOnCoefficients = Param(parent='undefined', name='lowerBound...\n","     |  \n","     |  lowerBoundsOnIntercepts = Param(parent='undefined', name='lowerBoundsO...\n","     |  \n","     |  threshold = Param(parent='undefined', name='threshold', doc=...s p, th...\n","     |  \n","     |  upperBoundsOnCoefficients = Param(parent='undefined', name='upperBound...\n","     |  \n","     |  upperBoundsOnIntercepts = Param(parent='undefined', name='upperBoundsO...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  getRegParam(self) -> float\n","     |      Gets the value of regParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasElasticNetParam:\n","     |  \n","     |  getElasticNetParam(self) -> float\n","     |      Gets the value of elasticNetParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasElasticNetParam:\n","     |  \n","     |  elasticNetParam = Param(parent='undefined', name='elasticNetParam'...L...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  getFitIntercept(self) -> bool\n","     |      Gets the value of fitIntercept or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  getStandardization(self) -> bool\n","     |      Gets the value of standardization or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  standardization = Param(parent='undefined', name='standardization'...t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  getAggregationDepth(self) -> int\n","     |      Gets the value of aggregationDepth or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  getMaxBlockSizeInMB(self) -> float\n","     |      Gets the value of maxBlockSizeInMB or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class LogisticRegressionModel(_JavaProbabilisticClassificationModel, _LogisticRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","     |  LogisticRegressionModel(*args, **kwds)\n","     |  \n","     |  Model fitted by LogisticRegression.\n","     |  \n","     |  .. versionadded:: 1.3.0\n","     |  \n","     |  Method resolution order:\n","     |      LogisticRegressionModel\n","     |      _JavaProbabilisticClassificationModel\n","     |      ProbabilisticClassificationModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _LogisticRegressionParams\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasRegParam\n","     |      pyspark.ml.param.shared.HasElasticNetParam\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasFitIntercept\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasStandardization\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.shared.HasAggregationDepth\n","     |      pyspark.ml.param.shared.HasThreshold\n","     |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      pyspark.ml.util.HasTrainingSummary\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  evaluate(self, dataset: pyspark.sql.dataframe.DataFrame) -> 'LogisticRegressionSummary'\n","     |      Evaluates the model on a test dataset.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          Test dataset to evaluate model on.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  coefficientMatrix\n","     |      Model coefficients.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  coefficients\n","     |      Model coefficients of binomial logistic regression.\n","     |      An exception is thrown in the case of multinomial logistic regression.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  intercept\n","     |      Model intercept of binomial logistic regression.\n","     |      An exception is thrown in the case of multinomial logistic regression.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  interceptVector\n","     |      Model intercept.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  summary\n","     |      Gets summary (accuracy/precision/recall, objective history, total iterations) of model\n","     |      trained on the training set. An exception is thrown if `trainingSummary is None`.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaProbabilisticClassificationModel:\n","     |  \n","     |  predictProbability(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Predict the probability of each class given the features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassificationModel:\n","     |  \n","     |  setProbabilityCol(self: ~CM, value: str) -> ~CM\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: ~CM, value: List[float]) -> ~CM\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _LogisticRegressionParams:\n","     |  \n","     |  getFamily(self) -> str\n","     |      Gets the value of :py:attr:`family` or its default value.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  getLowerBoundsOnCoefficients(self) -> pyspark.ml.linalg.Matrix\n","     |      Gets the value of :py:attr:`lowerBoundsOnCoefficients`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  getLowerBoundsOnIntercepts(self) -> pyspark.ml.linalg.Vector\n","     |      Gets the value of :py:attr:`lowerBoundsOnIntercepts`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  getThreshold(self) -> float\n","     |      Get threshold for binary classification.\n","     |      \n","     |      If :py:attr:`thresholds` is set with length 2 (i.e., binary classification),\n","     |      this returns the equivalent threshold:\n","     |      :math:`\\frac{1}{1 + \\frac{thresholds(0)}{thresholds(1)}}`.\n","     |      Otherwise, returns :py:attr:`threshold` if set or its default value if unset.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      If :py:attr:`thresholds` is set, return its value.\n","     |      Otherwise, if :py:attr:`threshold` is set, return the equivalent thresholds for binary\n","     |      classification: (1-threshold, threshold).\n","     |      If neither are set, throw an error.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  getUpperBoundsOnCoefficients(self) -> pyspark.ml.linalg.Matrix\n","     |      Gets the value of :py:attr:`upperBoundsOnCoefficients`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  getUpperBoundsOnIntercepts(self) -> pyspark.ml.linalg.Vector\n","     |      Gets the value of :py:attr:`upperBoundsOnIntercepts`\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |  \n","     |  setThreshold(self: 'P', value: float) -> 'P'\n","     |      Sets the value of :py:attr:`threshold`.\n","     |      Clears value of :py:attr:`thresholds` if it has been set.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _LogisticRegressionParams:\n","     |  \n","     |  __annotations__ = {'family': pyspark.ml.param.Param[str], 'lowerBounds...\n","     |  \n","     |  family = Param(parent='undefined', name='family', doc='Th... Supported...\n","     |  \n","     |  lowerBoundsOnCoefficients = Param(parent='undefined', name='lowerBound...\n","     |  \n","     |  lowerBoundsOnIntercepts = Param(parent='undefined', name='lowerBoundsO...\n","     |  \n","     |  threshold = Param(parent='undefined', name='threshold', doc=...s p, th...\n","     |  \n","     |  upperBoundsOnCoefficients = Param(parent='undefined', name='upperBound...\n","     |  \n","     |  upperBoundsOnIntercepts = Param(parent='undefined', name='upperBoundsO...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  getRegParam(self) -> float\n","     |      Gets the value of regParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n","     |  \n","     |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasElasticNetParam:\n","     |  \n","     |  getElasticNetParam(self) -> float\n","     |      Gets the value of elasticNetParam or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasElasticNetParam:\n","     |  \n","     |  elasticNetParam = Param(parent='undefined', name='elasticNetParam'...L...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  getFitIntercept(self) -> bool\n","     |      Gets the value of fitIntercept or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n","     |  \n","     |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  getStandardization(self) -> bool\n","     |      Gets the value of standardization or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n","     |  \n","     |  standardization = Param(parent='undefined', name='standardization'...t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  getAggregationDepth(self) -> int\n","     |      Gets the value of aggregationDepth or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n","     |  \n","     |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  getMaxBlockSizeInMB(self) -> float\n","     |      Gets the value of maxBlockSizeInMB or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n","     |  \n","     |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.util.HasTrainingSummary:\n","     |  \n","     |  hasSummary\n","     |      Indicates whether a training summary exists for this model\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class LogisticRegressionSummary(_ClassificationSummary)\n","     |  LogisticRegressionSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for Logistic Regression Results for a given model.\n","     |  \n","     |  .. versionadded:: 2.0.0\n","     |  \n","     |  Method resolution order:\n","     |      LogisticRegressionSummary\n","     |      _ClassificationSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties defined here:\n","     |  \n","     |  featuresCol\n","     |      Field in \"predictions\" which gives the features of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  probabilityCol\n","     |      Field in \"predictions\" which gives the probability\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class LogisticRegressionTrainingSummary(LogisticRegressionSummary, _TrainingSummary)\n","     |  LogisticRegressionTrainingSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for multinomial Logistic Regression Training results.\n","     |  \n","     |  .. versionadded:: 2.0.0\n","     |  \n","     |  Method resolution order:\n","     |      LogisticRegressionTrainingSummary\n","     |      LogisticRegressionSummary\n","     |      _ClassificationSummary\n","     |      _TrainingSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Readonly properties inherited from LogisticRegressionSummary:\n","     |  \n","     |  featuresCol\n","     |      Field in \"predictions\" which gives the features of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  probabilityCol\n","     |      Field in \"predictions\" which gives the probability\n","     |      of each class as a vector.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _TrainingSummary:\n","     |  \n","     |  objectiveHistory\n","     |      Objective function (scaled loss + regularization) at each\n","     |      iteration. It contains one more element, the initial state,\n","     |      than number of iterations.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  totalIterations\n","     |      Number of training iterations until termination.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class MultilayerPerceptronClassificationModel(_JavaProbabilisticClassificationModel, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","     |  MultilayerPerceptronClassificationModel(*args, **kwds)\n","     |  \n","     |  Model fitted by MultilayerPerceptronClassifier.\n","     |  \n","     |  .. versionadded:: 1.6.0\n","     |  \n","     |  Method resolution order:\n","     |      MultilayerPerceptronClassificationModel\n","     |      _JavaProbabilisticClassificationModel\n","     |      ProbabilisticClassificationModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _MultilayerPerceptronParams\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasStepSize\n","     |      pyspark.ml.param.shared.HasSolver\n","     |      pyspark.ml.param.shared.HasBlockSize\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      pyspark.ml.util.HasTrainingSummary\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  evaluate(self, dataset: pyspark.sql.dataframe.DataFrame) -> 'MultilayerPerceptronClassificationSummary'\n","     |      Evaluates the model on a test dataset.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          Test dataset to evaluate model on.\n","     |  \n","     |  summary(self) -> 'MultilayerPerceptronClassificationTrainingSummary'\n","     |      Gets summary (accuracy/precision/recall, objective history, total iterations) of model\n","     |      trained on the training set. An exception is thrown if `trainingSummary is None`.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  weights\n","     |      the weights of layers.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaProbabilisticClassificationModel:\n","     |  \n","     |  predictProbability(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Predict the probability of each class given the features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassificationModel:\n","     |  \n","     |  setProbabilityCol(self: ~CM, value: str) -> ~CM\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: ~CM, value: List[float]) -> ~CM\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _MultilayerPerceptronParams:\n","     |  \n","     |  getInitialWeights(self) -> pyspark.ml.linalg.Vector\n","     |      Gets the value of initialWeights or its default value.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  getLayers(self) -> List[int]\n","     |      Gets the value of layers or its default value.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _MultilayerPerceptronParams:\n","     |  \n","     |  __annotations__ = {'initialWeights': pyspark.ml.param.Param[pyspark.ml...\n","     |  \n","     |  initialWeights = Param(parent='undefined', name='initialWeights', doc=...\n","     |  \n","     |  layers = Param(parent='undefined', name='layers', doc='Si...ith 100 ne...\n","     |  \n","     |  solver = Param(parent='undefined', name='solver', doc='Th...or optimiz...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  getStepSize(self) -> float\n","     |      Gets the value of stepSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  stepSize = Param(parent='undefined', name='stepSize', doc='...used for...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n","     |  \n","     |  getSolver(self) -> str\n","     |      Gets the value of solver or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasBlockSize:\n","     |  \n","     |  getBlockSize(self) -> int\n","     |      Gets the value of blockSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasBlockSize:\n","     |  \n","     |  blockSize = Param(parent='undefined', name='blockSize', doc=...n then ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.util.HasTrainingSummary:\n","     |  \n","     |  hasSummary\n","     |      Indicates whether a training summary exists for this model\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class MultilayerPerceptronClassificationSummary(_ClassificationSummary)\n","     |  MultilayerPerceptronClassificationSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for MultilayerPerceptronClassifier Results for a given model.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      MultilayerPerceptronClassificationSummary\n","     |      _ClassificationSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class MultilayerPerceptronClassificationTrainingSummary(MultilayerPerceptronClassificationSummary, _TrainingSummary)\n","     |  MultilayerPerceptronClassificationTrainingSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for MultilayerPerceptronClassifier Training results.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      MultilayerPerceptronClassificationTrainingSummary\n","     |      MultilayerPerceptronClassificationSummary\n","     |      _ClassificationSummary\n","     |      _TrainingSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _TrainingSummary:\n","     |  \n","     |  objectiveHistory\n","     |      Objective function (scaled loss + regularization) at each\n","     |      iteration. It contains one more element, the initial state,\n","     |      than number of iterations.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  totalIterations\n","     |      Number of training iterations until termination.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class MultilayerPerceptronClassifier(_JavaProbabilisticClassifier, _MultilayerPerceptronParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  MultilayerPerceptronClassifier(*args, **kwds)\n","     |  \n","     |  Classifier trainer based on the Multilayer Perceptron.\n","     |  Each layer has sigmoid activation function, output layer has softmax.\n","     |  Number of inputs has to be equal to the size of feature vectors.\n","     |  Number of outputs has to be equal to the total number of labels.\n","     |  \n","     |  .. versionadded:: 1.6.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> df = spark.createDataFrame([\n","     |  ...     (0.0, Vectors.dense([0.0, 0.0])),\n","     |  ...     (1.0, Vectors.dense([0.0, 1.0])),\n","     |  ...     (1.0, Vectors.dense([1.0, 0.0])),\n","     |  ...     (0.0, Vectors.dense([1.0, 1.0]))], [\"label\", \"features\"])\n","     |  >>> mlp = MultilayerPerceptronClassifier(layers=[2, 2, 2], seed=123)\n","     |  >>> mlp.setMaxIter(100)\n","     |  MultilayerPerceptronClassifier...\n","     |  >>> mlp.getMaxIter()\n","     |  100\n","     |  >>> mlp.getBlockSize()\n","     |  128\n","     |  >>> mlp.setBlockSize(1)\n","     |  MultilayerPerceptronClassifier...\n","     |  >>> mlp.getBlockSize()\n","     |  1\n","     |  >>> model = mlp.fit(df)\n","     |  >>> model.setFeaturesCol(\"features\")\n","     |  MultilayerPerceptronClassificationModel...\n","     |  >>> model.getMaxIter()\n","     |  100\n","     |  >>> model.getLayers()\n","     |  [2, 2, 2]\n","     |  >>> model.weights.size\n","     |  12\n","     |  >>> testDF = spark.createDataFrame([\n","     |  ...     (Vectors.dense([1.0, 0.0]),),\n","     |  ...     (Vectors.dense([0.0, 0.0]),)], [\"features\"])\n","     |  >>> model.predict(testDF.head().features)\n","     |  1.0\n","     |  >>> model.predictRaw(testDF.head().features)\n","     |  DenseVector([-16.208, 16.344])\n","     |  >>> model.predictProbability(testDF.head().features)\n","     |  DenseVector([0.0, 1.0])\n","     |  >>> model.transform(testDF).select(\"features\", \"prediction\").show()\n","     |  +---------+----------+\n","     |  | features|prediction|\n","     |  +---------+----------+\n","     |  |[1.0,0.0]|       1.0|\n","     |  |[0.0,0.0]|       0.0|\n","     |  +---------+----------+\n","     |  ...\n","     |  >>> mlp_path = temp_path + \"/mlp\"\n","     |  >>> mlp.save(mlp_path)\n","     |  >>> mlp2 = MultilayerPerceptronClassifier.load(mlp_path)\n","     |  >>> mlp2.getBlockSize()\n","     |  1\n","     |  >>> model_path = temp_path + \"/mlp_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = MultilayerPerceptronClassificationModel.load(model_path)\n","     |  >>> model.getLayers() == model2.getLayers()\n","     |  True\n","     |  >>> model.weights == model2.weights\n","     |  True\n","     |  >>> model.transform(testDF).take(1) == model2.transform(testDF).take(1)\n","     |  True\n","     |  >>> mlp2 = mlp2.setInitialWeights(list(range(0, 12)))\n","     |  >>> model3 = mlp2.fit(df)\n","     |  >>> model3.weights != model2.weights\n","     |  True\n","     |  >>> model3.getLayers() == model.getLayers()\n","     |  True\n","     |  \n","     |  Method resolution order:\n","     |      MultilayerPerceptronClassifier\n","     |      _JavaProbabilisticClassifier\n","     |      ProbabilisticClassifier\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _MultilayerPerceptronParams\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasMaxIter\n","     |      pyspark.ml.param.shared.HasTol\n","     |      pyspark.ml.param.shared.HasStepSize\n","     |      pyspark.ml.param.shared.HasSolver\n","     |      pyspark.ml.param.shared.HasBlockSize\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, tol: float = 1e-06, seed: Union[int, NoneType] = None, layers: Union[List[int], NoneType] = None, blockSize: int = 128, stepSize: float = 0.03, solver: str = 'l-bfgs', initialWeights: Union[pyspark.ml.linalg.Vector, NoneType] = None, probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction')\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, tol=1e-6, seed=None, layers=None, blockSize=128, stepSize=0.03,                  solver=\"l-bfgs\", initialWeights=None, probabilityCol=\"probability\",                  rawPredictionCol=\"rawPrediction\")\n","     |  \n","     |  setBlockSize(self, value: int) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`blockSize`.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  setInitialWeights(self, value: pyspark.ml.linalg.Vector) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`initialWeights`.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  setLayers(self, value: List[int]) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`layers`.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  setMaxIter(self, value: int) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`maxIter`.\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, tol: float = 1e-06, seed: Union[int, NoneType] = None, layers: Union[List[int], NoneType] = None, blockSize: int = 128, stepSize: float = 0.03, solver: str = 'l-bfgs', initialWeights: Union[pyspark.ml.linalg.Vector, NoneType] = None, probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction') -> 'MultilayerPerceptronClassifier'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxIter=100, tol=1e-6, seed=None, layers=None, blockSize=128, stepSize=0.03,                   solver=\"l-bfgs\", initialWeights=None, probabilityCol=\"probability\",                   rawPredictionCol=\"rawPrediction\"):\n","     |      Sets params for MultilayerPerceptronClassifier.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  setSeed(self, value: int) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`seed`.\n","     |  \n","     |  setSolver(self, value: str) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`solver`.\n","     |  \n","     |  setStepSize(self, value: float) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`stepSize`.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  setTol(self, value: float) -> 'MultilayerPerceptronClassifier'\n","     |      Sets the value of :py:attr:`tol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClas...r...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassifier:\n","     |  \n","     |  setProbabilityCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: 'P', value: List[float]) -> 'P'\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _MultilayerPerceptronParams:\n","     |  \n","     |  getInitialWeights(self) -> pyspark.ml.linalg.Vector\n","     |      Gets the value of initialWeights or its default value.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  getLayers(self) -> List[int]\n","     |      Gets the value of layers or its default value.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _MultilayerPerceptronParams:\n","     |  \n","     |  initialWeights = Param(parent='undefined', name='initialWeights', doc=...\n","     |  \n","     |  layers = Param(parent='undefined', name='layers', doc='Si...ith 100 ne...\n","     |  \n","     |  solver = Param(parent='undefined', name='solver', doc='Th...or optimiz...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  getMaxIter(self) -> int\n","     |      Gets the value of maxIter or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n","     |  \n","     |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  getTol(self) -> float\n","     |      Gets the value of tol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n","     |  \n","     |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  getStepSize(self) -> float\n","     |      Gets the value of stepSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasStepSize:\n","     |  \n","     |  stepSize = Param(parent='undefined', name='stepSize', doc='...used for...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n","     |  \n","     |  getSolver(self) -> str\n","     |      Gets the value of solver or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasBlockSize:\n","     |  \n","     |  getBlockSize(self) -> int\n","     |      Gets the value of blockSize or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasBlockSize:\n","     |  \n","     |  blockSize = Param(parent='undefined', name='blockSize', doc=...n then ...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class NaiveBayes(_JavaProbabilisticClassifier, _NaiveBayesParams, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  NaiveBayes(*args, **kwds)\n","     |  \n","     |  Naive Bayes Classifiers.\n","     |  It supports both Multinomial and Bernoulli NB. `Multinomial NB     <http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html>`_\n","     |  can handle finitely supported discrete data. For example, by converting documents into\n","     |  TF-IDF vectors, it can be used for document classification. By making every vector a\n","     |  binary (0/1) data, it can also be used as `Bernoulli NB     <http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html>`_.\n","     |  \n","     |  The input feature values for Multinomial NB and Bernoulli NB must be nonnegative.\n","     |  Since 3.0.0, it supports Complement NB which is an adaptation of the Multinomial NB.\n","     |  Specifically, Complement NB uses statistics from the complement of each class to compute\n","     |  the model's coefficients. The inventors of Complement NB show empirically that the parameter\n","     |  estimates for CNB are more stable than those for Multinomial NB. Like Multinomial NB, the\n","     |  input feature values for Complement NB must be nonnegative.\n","     |  Since 3.0.0, it also supports `Gaussian NB     <https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes>`_.\n","     |  which can handle continuous data.\n","     |  \n","     |  .. versionadded:: 1.5.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.sql import Row\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> df = spark.createDataFrame([\n","     |  ...     Row(label=0.0, weight=0.1, features=Vectors.dense([0.0, 0.0])),\n","     |  ...     Row(label=0.0, weight=0.5, features=Vectors.dense([0.0, 1.0])),\n","     |  ...     Row(label=1.0, weight=1.0, features=Vectors.dense([1.0, 0.0]))])\n","     |  >>> nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", weightCol=\"weight\")\n","     |  >>> model = nb.fit(df)\n","     |  >>> model.setFeaturesCol(\"features\")\n","     |  NaiveBayesModel...\n","     |  >>> model.getSmoothing()\n","     |  1.0\n","     |  >>> model.pi\n","     |  DenseVector([-0.81..., -0.58...])\n","     |  >>> model.theta\n","     |  DenseMatrix(2, 2, [-0.91..., -0.51..., -0.40..., -1.09...], 1)\n","     |  >>> model.sigma\n","     |  DenseMatrix(0, 0, [...], ...)\n","     |  >>> test0 = sc.parallelize([Row(features=Vectors.dense([1.0, 0.0]))]).toDF()\n","     |  >>> model.predict(test0.head().features)\n","     |  1.0\n","     |  >>> model.predictRaw(test0.head().features)\n","     |  DenseVector([-1.72..., -0.99...])\n","     |  >>> model.predictProbability(test0.head().features)\n","     |  DenseVector([0.32..., 0.67...])\n","     |  >>> result = model.transform(test0).head()\n","     |  >>> result.prediction\n","     |  1.0\n","     |  >>> result.probability\n","     |  DenseVector([0.32..., 0.67...])\n","     |  >>> result.rawPrediction\n","     |  DenseVector([-1.72..., -0.99...])\n","     |  >>> test1 = sc.parallelize([Row(features=Vectors.sparse(2, [0], [1.0]))]).toDF()\n","     |  >>> model.transform(test1).head().prediction\n","     |  1.0\n","     |  >>> nb_path = temp_path + \"/nb\"\n","     |  >>> nb.save(nb_path)\n","     |  >>> nb2 = NaiveBayes.load(nb_path)\n","     |  >>> nb2.getSmoothing()\n","     |  1.0\n","     |  >>> model_path = temp_path + \"/nb_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = NaiveBayesModel.load(model_path)\n","     |  >>> model.pi == model2.pi\n","     |  True\n","     |  >>> model.theta == model2.theta\n","     |  True\n","     |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  >>> nb = nb.setThresholds([0.01, 10.00])\n","     |  >>> model3 = nb.fit(df)\n","     |  >>> result = model3.transform(test0).head()\n","     |  >>> result.prediction\n","     |  0.0\n","     |  >>> nb3 = NaiveBayes().setModelType(\"gaussian\")\n","     |  >>> model4 = nb3.fit(df)\n","     |  >>> model4.getModelType()\n","     |  'gaussian'\n","     |  >>> model4.sigma\n","     |  DenseMatrix(2, 2, [0.0, 0.25, 0.0, 0.0], 1)\n","     |  >>> nb5 = NaiveBayes(smoothing=1.0, modelType=\"complement\", weightCol=\"weight\")\n","     |  >>> model5 = nb5.fit(df)\n","     |  >>> model5.getModelType()\n","     |  'complement'\n","     |  >>> model5.theta\n","     |  DenseMatrix(2, 2, [...], 1)\n","     |  >>> model5.sigma\n","     |  DenseMatrix(0, 0, [...], ...)\n","     |  \n","     |  Method resolution order:\n","     |      NaiveBayes\n","     |      _JavaProbabilisticClassifier\n","     |      ProbabilisticClassifier\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      _NaiveBayesParams\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', smoothing: float = 1.0, modelType: str = 'multinomial', thresholds: Union[List[float], NoneType] = None, weightCol: Union[str, NoneType] = None)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0,                  modelType=\"multinomial\", thresholds=None, weightCol=None)\n","     |  \n","     |  setModelType(self, value: str) -> 'NaiveBayes'\n","     |      Sets the value of :py:attr:`modelType`.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', smoothing: float = 1.0, modelType: str = 'multinomial', thresholds: Union[List[float], NoneType] = None, weightCol: Union[str, NoneType] = None) -> 'NaiveBayes'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0,                   modelType=\"multinomial\", thresholds=None, weightCol=None)\n","     |      Sets params for Naive Bayes.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  setSmoothing(self, value: float) -> 'NaiveBayes'\n","     |      Sets the value of :py:attr:`smoothing`.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  setWeightCol(self, value: str) -> 'NaiveBayes'\n","     |      Sets the value of :py:attr:`weightCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassifier:\n","     |  \n","     |  setProbabilityCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: 'P', value: List[float]) -> 'P'\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _NaiveBayesParams:\n","     |  \n","     |  getModelType(self) -> str\n","     |      Gets the value of modelType or its default value.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  getSmoothing(self) -> float\n","     |      Gets the value of smoothing or its default value.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _NaiveBayesParams:\n","     |  \n","     |  modelType = Param(parent='undefined', name='modelType', doc=... multin...\n","     |  \n","     |  smoothing = Param(parent='undefined', name='smoothing', doc=...thing p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class NaiveBayesModel(_JavaProbabilisticClassificationModel, _NaiveBayesParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  NaiveBayesModel(*args, **kwds)\n","     |  \n","     |  Model fitted by NaiveBayes.\n","     |  \n","     |  .. versionadded:: 1.5.0\n","     |  \n","     |  Method resolution order:\n","     |      NaiveBayesModel\n","     |      _JavaProbabilisticClassificationModel\n","     |      ProbabilisticClassificationModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      _NaiveBayesParams\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Readonly properties defined here:\n","     |  \n","     |  pi\n","     |      log of class priors.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  sigma\n","     |      variance of each feature.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  theta\n","     |      log of class conditional probabilities.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaProbabilisticClassificationModel:\n","     |  \n","     |  predictProbability(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Predict the probability of each class given the features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassificationModel:\n","     |  \n","     |  setProbabilityCol(self: ~CM, value: str) -> ~CM\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: ~CM, value: List[float]) -> ~CM\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  __annotations__ = {'probabilityCol': 'Param[str]'}\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _NaiveBayesParams:\n","     |  \n","     |  getModelType(self) -> str\n","     |      Gets the value of modelType or its default value.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  getSmoothing(self) -> float\n","     |      Gets the value of smoothing or its default value.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _NaiveBayesParams:\n","     |  \n","     |  modelType = Param(parent='undefined', name='modelType', doc=... multin...\n","     |  \n","     |  smoothing = Param(parent='undefined', name='smoothing', doc=...thing p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class OneVsRest(pyspark.ml.base.Estimator, _OneVsRestParams, pyspark.ml.param.shared.HasParallelism, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable, typing.Generic)\n","     |  OneVsRest(*args, **kwds)\n","     |  \n","     |  Reduction of Multiclass Classification to Binary Classification.\n","     |  Performs reduction using one against all strategy.\n","     |  For a multiclass classification with k classes, train k models (one per class).\n","     |  Each example is scored against all k models and the model with highest score\n","     |  is picked to label the example.\n","     |  \n","     |  .. versionadded:: 2.0.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.sql import Row\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> data_path = \"data/mllib/sample_multiclass_classification_data.txt\"\n","     |  >>> df = spark.read.format(\"libsvm\").load(data_path)\n","     |  >>> lr = LogisticRegression(regParam=0.01)\n","     |  >>> ovr = OneVsRest(classifier=lr)\n","     |  >>> ovr.getRawPredictionCol()\n","     |  'rawPrediction'\n","     |  >>> ovr.setPredictionCol(\"newPrediction\")\n","     |  OneVsRest...\n","     |  >>> model = ovr.fit(df)\n","     |  >>> model.models[0].coefficients\n","     |  DenseVector([0.5..., -1.0..., 3.4..., 4.2...])\n","     |  >>> model.models[1].coefficients\n","     |  DenseVector([-2.1..., 3.1..., -2.6..., -2.3...])\n","     |  >>> model.models[2].coefficients\n","     |  DenseVector([0.3..., -3.4..., 1.0..., -1.1...])\n","     |  >>> [x.intercept for x in model.models]\n","     |  [-2.7..., -2.5..., -1.3...]\n","     |  >>> test0 = sc.parallelize([Row(features=Vectors.dense(-1.0, 0.0, 1.0, 1.0))]).toDF()\n","     |  >>> model.transform(test0).head().newPrediction\n","     |  0.0\n","     |  >>> test1 = sc.parallelize([Row(features=Vectors.sparse(4, [0], [1.0]))]).toDF()\n","     |  >>> model.transform(test1).head().newPrediction\n","     |  2.0\n","     |  >>> test2 = sc.parallelize([Row(features=Vectors.dense(0.5, 0.4, 0.3, 0.2))]).toDF()\n","     |  >>> model.transform(test2).head().newPrediction\n","     |  0.0\n","     |  >>> model_path = temp_path + \"/ovr_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = OneVsRestModel.load(model_path)\n","     |  >>> model2.transform(test0).head().newPrediction\n","     |  0.0\n","     |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  >>> model.transform(test2).columns\n","     |  ['features', 'rawPrediction', 'newPrediction']\n","     |  \n","     |  Method resolution order:\n","     |      OneVsRest\n","     |      pyspark.ml.base.Estimator\n","     |      _OneVsRestParams\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.shared.HasParallelism\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.MLReadable\n","     |      pyspark.ml.util.MLWritable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', rawPredictionCol: str = 'rawPrediction', classifier: Union[pyspark.ml.classification.Classifier[~CM], NoneType] = None, weightCol: Union[str, NoneType] = None, parallelism: int = 1)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  rawPredictionCol=\"rawPrediction\", classifier=None, weightCol=None, parallelism=1):\n","     |  \n","     |  copy(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'OneVsRest'\n","     |      Creates a copy of this instance with a randomly generated uid\n","     |      and some extra params. This creates a deep copy of the embedded paramMap,\n","     |      and copies the embedded and extra parameters over.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |      \n","     |      Examples\n","     |      --------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`OneVsRest`\n","     |          Copy of this instance\n","     |  \n","     |  setClassifier(self, value: pyspark.ml.classification.Classifier[~CM]) -> 'OneVsRest'\n","     |      Sets the value of :py:attr:`classifier`.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  setFeaturesCol(self, value: str) -> 'OneVsRest'\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |  \n","     |  setLabelCol(self, value: str) -> 'OneVsRest'\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |  \n","     |  setParallelism(self, value: int) -> 'OneVsRest'\n","     |      Sets the value of :py:attr:`parallelism`.\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', rawPredictionCol: str = 'rawPrediction', classifier: Union[pyspark.ml.classification.Classifier[~CM], NoneType] = None, weightCol: Union[str, NoneType] = None, parallelism: int = 1) -> 'OneVsRest'\n","     |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   rawPredictionCol=\"rawPrediction\", classifier=None, weightCol=None, parallelism=1):\n","     |      Sets params for OneVsRest.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  setPredictionCol(self, value: str) -> 'OneVsRest'\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |  \n","     |  setRawPredictionCol(self, value: str) -> 'OneVsRest'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |  \n","     |  setWeightCol(self, value: str) -> 'OneVsRest'\n","     |      Sets the value of :py:attr:`weightCol`.\n","     |  \n","     |  write(self) -> pyspark.ml.util.MLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  read() -> 'OneVsRestReader' from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.base.Estimator[ForwardRef('OneVsRestModel...\n","     |  \n","     |  __parameters__ = (~CM,)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _OneVsRestParams:\n","     |  \n","     |  getClassifier(self) -> pyspark.ml.classification.Classifier\n","     |      Gets the value of classifier or its default value.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _OneVsRestParams:\n","     |  \n","     |  classifier = Param(parent='undefined', name='classifier', doc='base bi...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasParallelism:\n","     |  \n","     |  getParallelism(self) -> int\n","     |      Gets the value of parallelism or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasParallelism:\n","     |  \n","     |  parallelism = Param(parent='undefined', name='parallelism', do...to us...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class OneVsRestModel(pyspark.ml.base.Model, _OneVsRestParams, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable)\n","     |  OneVsRestModel(*args, **kwds)\n","     |  \n","     |  Model fitted by OneVsRest.\n","     |  This stores the models resulting from training k binary classifiers: one for each class.\n","     |  Each example is scored against all k models, and the model with the highest score\n","     |  is picked to label the example.\n","     |  \n","     |  .. versionadded:: 2.0.0\n","     |  \n","     |  Method resolution order:\n","     |      OneVsRestModel\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _OneVsRestParams\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      pyspark.ml.util.MLWritable\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, models: List[pyspark.ml.classification.ClassificationModel])\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  copy(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'OneVsRestModel'\n","     |      Creates a copy of this instance with a randomly generated uid\n","     |      and some extra params. This creates a deep copy of the embedded paramMap,\n","     |      and copies the embedded and extra parameters over.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`OneVsRestModel`\n","     |          Copy of this instance\n","     |  \n","     |  setFeaturesCol(self, value: str) -> 'OneVsRestModel'\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |  \n","     |  setPredictionCol(self, value: str) -> 'OneVsRestModel'\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |  \n","     |  setRawPredictionCol(self, value: str) -> 'OneVsRestModel'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |  \n","     |  write(self) -> pyspark.ml.util.MLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  read() -> 'OneVsRestModelReader' from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (<class 'pyspark.ml.base.Model'>, <class 'pyspark.ml....\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _OneVsRestParams:\n","     |  \n","     |  getClassifier(self) -> pyspark.ml.classification.Classifier\n","     |      Gets the value of classifier or its default value.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from _OneVsRestParams:\n","     |  \n","     |  __annotations__ = {'classifier': pyspark.ml.param.Param[pyspark.ml.cla...\n","     |  \n","     |  classifier = Param(parent='undefined', name='classifier', doc='base bi...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","    \n","    class RandomForestClassificationModel(pyspark.ml.tree._TreeEnsembleModel, _JavaProbabilisticClassificationModel, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.HasTrainingSummary)\n","     |  RandomForestClassificationModel(*args, **kwds)\n","     |  \n","     |  Model fitted by RandomForestClassifier.\n","     |  \n","     |  .. versionadded:: 1.4.0\n","     |  \n","     |  Method resolution order:\n","     |      RandomForestClassificationModel\n","     |      pyspark.ml.tree._TreeEnsembleModel\n","     |      _JavaProbabilisticClassificationModel\n","     |      ProbabilisticClassificationModel\n","     |      _JavaClassificationModel\n","     |      ClassificationModel\n","     |      pyspark.ml.wrapper.JavaPredictionModel\n","     |      pyspark.ml.base.PredictionModel\n","     |      pyspark.ml.wrapper.JavaModel\n","     |      pyspark.ml.wrapper.JavaTransformer\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Model\n","     |      pyspark.ml.base.Transformer\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      _RandomForestClassifierParams\n","     |      pyspark.ml.tree._RandomForestParams\n","     |      pyspark.ml.tree._TreeEnsembleParams\n","     |      pyspark.ml.tree._DecisionTreeParams\n","     |      pyspark.ml.param.shared.HasCheckpointInterval\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.tree._TreeClassifierParams\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      pyspark.ml.util.HasTrainingSummary\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  evaluate(self, dataset: pyspark.sql.dataframe.DataFrame) -> Union[ForwardRef('BinaryRandomForestClassificationSummary'), ForwardRef('RandomForestClassificationSummary')]\n","     |      Evaluates the model on a test dataset.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          Test dataset to evaluate model on.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  featureImportances\n","     |      Estimate of the importance of each feature.\n","     |      \n","     |      Each feature's importance is the average of its importance across all trees in the ensemble\n","     |      The importance vector is normalized to sum to 1. This method is suggested by Hastie et al.\n","     |      (Hastie, Tibshirani, Friedman. \"The Elements of Statistical Learning, 2nd Edition.\" 2001.)\n","     |      and follows the implementation from scikit-learn.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      DecisionTreeClassificationModel.featureImportances\n","     |  \n","     |  summary\n","     |      Gets summary (accuracy/precision/recall, objective history, total iterations) of model\n","     |      trained on the training set. An exception is thrown if `trainingSummary is None`.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  trees\n","     |      Trees in this ensemble. Warning: These have null parent Estimators.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __orig_bases__ = (<class 'pyspark.ml.tree._TreeEnsembleModel'>, pyspar...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeEnsembleModel:\n","     |  \n","     |  predictLeaf(self, value: pyspark.ml.linalg.Vector) -> float\n","     |      Predict the indices of the leaves corresponding to the feature vector.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.tree._TreeEnsembleModel:\n","     |  \n","     |  getNumTrees\n","     |      Number of trees in ensemble.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  toDebugString\n","     |      Full description of model.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  totalNumNodes\n","     |      Total number of nodes, summed over all trees in the ensemble.\n","     |      \n","     |      .. versionadded:: 2.0.0\n","     |  \n","     |  treeWeights\n","     |      Return the weights for each tree\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaProbabilisticClassificationModel:\n","     |  \n","     |  predictProbability(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Predict the probability of each class given the features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassificationModel:\n","     |  \n","     |  setProbabilityCol(self: ~CM, value: str) -> ~CM\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: ~CM, value: List[float]) -> ~CM\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassificationModel:\n","     |  \n","     |  predictRaw(self, value: pyspark.ml.linalg.Vector) -> pyspark.ml.linalg.Vector\n","     |      Raw prediction for each possible label.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _JavaClassificationModel:\n","     |  \n","     |  numClasses\n","     |      Number of classes (values which the label can take).\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ClassificationModel:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  predict(self, value: ~T) -> float\n","     |      Predict label for the given features.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.wrapper.JavaPredictionModel:\n","     |  \n","     |  numFeatures\n","     |      Returns the number of features the model was trained on. If unknown, returns -1\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.PredictionModel:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaModel:\n","     |  \n","     |  __init__(self, java_model: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize this instance with a Java model object.\n","     |      Subclasses should call this constructor, initialize params,\n","     |      and then call _transfer_params_from_java.\n","     |      \n","     |      This instance can be instantiated without specifying java_model,\n","     |      it will be assigned after that, but this scenario only used by\n","     |      :py:class:`JavaMLReader` to load models.  This is a bit of a\n","     |      hack, but it is easiest since a proper fix would require\n","     |      MLReader (in pyspark.ml.util) to depend on these wrappers, but\n","     |      these wrappers depend on pyspark.ml.util (both directly and via\n","     |      other ML classes).\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Transformer:\n","     |  \n","     |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n","     |      Transforms the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset\n","     |      params : dict, optional\n","     |          an optional param map that overrides embedded params.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`pyspark.sql.DataFrame`\n","     |          transformed dataset\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  __annotations__ = {'probabilityCol': 'Param[str]'}\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._RandomForestParams:\n","     |  \n","     |  getBootstrap(self) -> bool\n","     |      Gets the value of bootstrap or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._RandomForestParams:\n","     |  \n","     |  bootstrap = Param(parent='undefined', name='bootstrap', doc=...bootstr...\n","     |  \n","     |  numTrees = Param(parent='undefined', name='numTrees', doc='Number of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  getFeatureSubsetStrategy(self) -> str\n","     |      Gets the value of featureSubsetStrategy or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  getSubsamplingRate(self) -> float\n","     |      Gets the value of subsamplingRate or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n","     |  \n","     |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n","     |  \n","     |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  getCacheNodeIds(self) -> bool\n","     |      Gets the value of cacheNodeIds or its default value.\n","     |  \n","     |  getLeafCol(self) -> str\n","     |      Gets the value of leafCol or its default value.\n","     |  \n","     |  getMaxBins(self) -> int\n","     |      Gets the value of maxBins or its default value.\n","     |  \n","     |  getMaxDepth(self) -> int\n","     |      Gets the value of maxDepth or its default value.\n","     |  \n","     |  getMaxMemoryInMB(self) -> int\n","     |      Gets the value of maxMemoryInMB or its default value.\n","     |  \n","     |  getMinInfoGain(self) -> float\n","     |      Gets the value of minInfoGain or its default value.\n","     |  \n","     |  getMinInstancesPerNode(self) -> int\n","     |      Gets the value of minInstancesPerNode or its default value.\n","     |  \n","     |  getMinWeightFractionPerNode(self) -> float\n","     |      Gets the value of minWeightFractionPerNode or its default value.\n","     |  \n","     |  setLeafCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`leafCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n","     |  \n","     |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n","     |  \n","     |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n","     |  \n","     |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n","     |  \n","     |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n","     |  \n","     |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n","     |  \n","     |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n","     |  \n","     |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  getCheckpointInterval(self) -> int\n","     |      Gets the value of checkpointInterval or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  getImpurity(self) -> str\n","     |      Gets the value of impurity or its default value.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  impurity = Param(parent='undefined', name='impurity', doc='...-insensi...\n","     |  \n","     |  supportedImpurities = ['entropy', 'gini']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.util.HasTrainingSummary:\n","     |  \n","     |  hasSummary\n","     |      Indicates whether a training summary exists for this model\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","    \n","    class RandomForestClassificationSummary(_ClassificationSummary)\n","     |  RandomForestClassificationSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for RandomForestClassification Results for a given model.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      RandomForestClassificationSummary\n","     |      _ClassificationSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class RandomForestClassificationTrainingSummary(RandomForestClassificationSummary, _TrainingSummary)\n","     |  RandomForestClassificationTrainingSummary(java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |  \n","     |  Abstraction for RandomForestClassificationTraining Training results.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Method resolution order:\n","     |      RandomForestClassificationTrainingSummary\n","     |      RandomForestClassificationSummary\n","     |      _ClassificationSummary\n","     |      _TrainingSummary\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      builtins.object\n","     |  \n","     |  Methods inherited from _ClassificationSummary:\n","     |  \n","     |  fMeasureByLabel(self, beta: float = 1.0) -> List[float]\n","     |      Returns f-measure for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFMeasure(self, beta: float = 1.0) -> float\n","     |      Returns weighted averaged f-measure.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _ClassificationSummary:\n","     |  \n","     |  accuracy\n","     |      Returns accuracy.\n","     |      (equals to the total number of correctly classified instances\n","     |      out of the total number of instances.)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  falsePositiveRateByLabel\n","     |      Returns false positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labelCol\n","     |      Field in \"predictions\" which gives the true label of each\n","     |      instance.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  labels\n","     |      Returns the sequence of labels in ascending order. This order matches the order used\n","     |      in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      In most cases, it will be values {0.0, 1.0, ..., numClasses-1}, However, if the\n","     |      training set is missing a label, then all of the arrays over labels\n","     |      (e.g., from truePositiveRateByLabel) will be of length numClasses-1 instead of the\n","     |      expected numClasses.\n","     |  \n","     |  precisionByLabel\n","     |      Returns precision for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictionCol\n","     |      Field in \"predictions\" which gives the prediction of each class.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  predictions\n","     |      Dataframe outputted by the model's `transform` method.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  recallByLabel\n","     |      Returns recall for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  truePositiveRateByLabel\n","     |      Returns true positive rate for each label (category).\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightCol\n","     |      Field in \"predictions\" which gives the weight of each instance\n","     |      as a vector.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedFalsePositiveRate\n","     |      Returns weighted false positive rate.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedPrecision\n","     |      Returns weighted averaged precision.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedRecall\n","     |      Returns weighted averaged recall.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  weightedTruePositiveRate\n","     |      Returns weighted true positive rate.\n","     |      (equals to precision, recall and f-measure)\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from _TrainingSummary:\n","     |  \n","     |  objectiveHistory\n","     |      Objective function (scaled loss + regularization) at each\n","     |      iteration. It contains one more element, the initial state,\n","     |      than number of iterations.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  totalIterations\n","     |      Number of training iterations until termination.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  __init__(self, java_obj: Union[ForwardRef('JavaObject'), NoneType] = None)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","    \n","    class RandomForestClassifier(_JavaProbabilisticClassifier, _RandomForestClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n","     |  RandomForestClassifier(*args, **kwds)\n","     |  \n","     |  `Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_\n","     |  learning algorithm for classification.\n","     |  It supports both binary and multiclass labels, as well as both continuous and categorical\n","     |  features.\n","     |  \n","     |  .. versionadded:: 1.4.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> import numpy\n","     |  >>> from numpy import allclose\n","     |  >>> from pyspark.ml.linalg import Vectors\n","     |  >>> from pyspark.ml.feature import StringIndexer\n","     |  >>> df = spark.createDataFrame([\n","     |  ...     (1.0, Vectors.dense(1.0)),\n","     |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n","     |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n","     |  >>> si_model = stringIndexer.fit(df)\n","     |  >>> td = si_model.transform(df)\n","     |  >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"indexed\", seed=42,\n","     |  ...     leafCol=\"leafId\")\n","     |  >>> rf.getMinWeightFractionPerNode()\n","     |  0.0\n","     |  >>> model = rf.fit(td)\n","     |  >>> model.getLabelCol()\n","     |  'indexed'\n","     |  >>> model.setFeaturesCol(\"features\")\n","     |  RandomForestClassificationModel...\n","     |  >>> model.setRawPredictionCol(\"newRawPrediction\")\n","     |  RandomForestClassificationModel...\n","     |  >>> model.getBootstrap()\n","     |  True\n","     |  >>> model.getRawPredictionCol()\n","     |  'newRawPrediction'\n","     |  >>> model.featureImportances\n","     |  SparseVector(1, {0: 1.0})\n","     |  >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])\n","     |  True\n","     |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n","     |  >>> model.predict(test0.head().features)\n","     |  0.0\n","     |  >>> model.predictRaw(test0.head().features)\n","     |  DenseVector([2.0, 0.0])\n","     |  >>> model.predictProbability(test0.head().features)\n","     |  DenseVector([1.0, 0.0])\n","     |  >>> result = model.transform(test0).head()\n","     |  >>> result.prediction\n","     |  0.0\n","     |  >>> numpy.argmax(result.probability)\n","     |  0\n","     |  >>> numpy.argmax(result.newRawPrediction)\n","     |  0\n","     |  >>> result.leafId\n","     |  DenseVector([0.0, 0.0, 0.0])\n","     |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n","     |  >>> model.transform(test1).head().prediction\n","     |  1.0\n","     |  >>> model.trees\n","     |  [DecisionTreeClassificationModel...depth=..., DecisionTreeClassificationModel...]\n","     |  >>> rfc_path = temp_path + \"/rfc\"\n","     |  >>> rf.save(rfc_path)\n","     |  >>> rf2 = RandomForestClassifier.load(rfc_path)\n","     |  >>> rf2.getNumTrees()\n","     |  3\n","     |  >>> model_path = temp_path + \"/rfc_model\"\n","     |  >>> model.save(model_path)\n","     |  >>> model2 = RandomForestClassificationModel.load(model_path)\n","     |  >>> model.featureImportances == model2.featureImportances\n","     |  True\n","     |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n","     |  True\n","     |  \n","     |  Method resolution order:\n","     |      RandomForestClassifier\n","     |      _JavaProbabilisticClassifier\n","     |      ProbabilisticClassifier\n","     |      _JavaClassifier\n","     |      Classifier\n","     |      pyspark.ml.wrapper.JavaPredictor\n","     |      pyspark.ml.base.Predictor\n","     |      pyspark.ml.wrapper.JavaEstimator\n","     |      pyspark.ml.wrapper.JavaParams\n","     |      pyspark.ml.wrapper.JavaWrapper\n","     |      pyspark.ml.base.Estimator\n","     |      _ProbabilisticClassifierParams\n","     |      pyspark.ml.param.shared.HasProbabilityCol\n","     |      pyspark.ml.param.shared.HasThresholds\n","     |      _ClassifierParams\n","     |      pyspark.ml.param.shared.HasRawPredictionCol\n","     |      pyspark.ml.base._PredictorParams\n","     |      pyspark.ml.param.shared.HasLabelCol\n","     |      pyspark.ml.param.shared.HasFeaturesCol\n","     |      pyspark.ml.param.shared.HasPredictionCol\n","     |      _RandomForestClassifierParams\n","     |      pyspark.ml.tree._RandomForestParams\n","     |      pyspark.ml.tree._TreeEnsembleParams\n","     |      pyspark.ml.tree._DecisionTreeParams\n","     |      pyspark.ml.param.shared.HasCheckpointInterval\n","     |      pyspark.ml.param.shared.HasSeed\n","     |      pyspark.ml.param.shared.HasWeightCol\n","     |      pyspark.ml.tree._TreeClassifierParams\n","     |      pyspark.ml.param.Params\n","     |      pyspark.ml.util.Identifiable\n","     |      pyspark.ml.util.JavaMLWritable\n","     |      pyspark.ml.util.MLWritable\n","     |      pyspark.ml.util.JavaMLReadable\n","     |      pyspark.ml.util.MLReadable\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, impurity: str = 'gini', numTrees: int = 20, featureSubsetStrategy: str = 'auto', seed: Union[int, NoneType] = None, subsamplingRate: float = 1.0, leafCol: str = '', minWeightFractionPerNode: float = 0.0, weightCol: Union[str, NoneType] = None, bootstrap: Union[bool, NoneType] = True)\n","     |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\",                  numTrees=20, featureSubsetStrategy=\"auto\", seed=None, subsamplingRate=1.0,                  leafCol=\"\", minWeightFractionPerNode=0.0, weightCol=None, bootstrap=True)\n","     |  \n","     |  setBootstrap(self, value: bool) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`bootstrap`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setCacheNodeIds(self, value: bool) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`cacheNodeIds`.\n","     |  \n","     |  setCheckpointInterval(self, value: int) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`checkpointInterval`.\n","     |  \n","     |  setFeatureSubsetStrategy(self, value: str) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`featureSubsetStrategy`.\n","     |      \n","     |      .. versionadded:: 2.4.0\n","     |  \n","     |  setImpurity(self, value: str) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`impurity`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setMaxBins(self, value: int) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`maxBins`.\n","     |  \n","     |  setMaxDepth(self, value: int) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`maxDepth`.\n","     |  \n","     |  setMaxMemoryInMB(self, value: int) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`maxMemoryInMB`.\n","     |  \n","     |  setMinInfoGain(self, value: float) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`minInfoGain`.\n","     |  \n","     |  setMinInstancesPerNode(self, value: int) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`minInstancesPerNode`.\n","     |  \n","     |  setMinWeightFractionPerNode(self, value: float) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`minWeightFractionPerNode`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setNumTrees(self, value: int) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`numTrees`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', probabilityCol: str = 'probability', rawPredictionCol: str = 'rawPrediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, impurity: str = 'gini', numTrees: int = 20, featureSubsetStrategy: str = 'auto', seed: Union[int, NoneType] = None, subsamplingRate: float = 1.0, leafCol: str = '', minWeightFractionPerNode: float = 0.0, weightCol: Union[str, NoneType] = None, bootstrap: Union[bool, NoneType] = True) -> 'RandomForestClassifier'\n","     |      setParams(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, seed=None,                   impurity=\"gini\", numTrees=20, featureSubsetStrategy=\"auto\", subsamplingRate=1.0,                   leafCol=\"\", minWeightFractionPerNode=0.0, weightCol=None, bootstrap=True)\n","     |      Sets params for linear classification.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setSeed(self, value: int) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`seed`.\n","     |  \n","     |  setSubsamplingRate(self, value: float) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`subsamplingRate`.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  setWeightCol(self, value: str) -> 'RandomForestClassifier'\n","     |      Sets the value of :py:attr:`weightCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __abstractmethods__ = frozenset()\n","     |  \n","     |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n","     |  \n","     |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClas...e...\n","     |  \n","     |  __parameters__ = ()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from ProbabilisticClassifier:\n","     |  \n","     |  setProbabilityCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`probabilityCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setThresholds(self: 'P', value: List[float]) -> 'P'\n","     |      Sets the value of :py:attr:`thresholds`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from _JavaClassifier:\n","     |  \n","     |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`rawPredictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Predictor:\n","     |  \n","     |  setFeaturesCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`featuresCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setLabelCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`labelCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  setPredictionCol(self: ~P, value: str) -> ~P\n","     |      Sets the value of :py:attr:`predictionCol`.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n","     |  \n","     |  clear(self, param: pyspark.ml.param.Param) -> None\n","     |      Clears a param from the param map if it has been explicitly set.\n","     |  \n","     |  copy(self: 'JP', extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'JP'\n","     |      Creates a copy of this instance with the same uid and some\n","     |      extra params. This implementation first calls Params.copy and\n","     |      then make a copy of the companion Java pipeline component with\n","     |      extra params. So both the Python wrapper and the Java pipeline\n","     |      component get copied.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          Extra parameters to copy to the new instance\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`JavaParams`\n","     |          Copy of this instance\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __del__(self) -> None\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables (if defined)\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object (if defined)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.base.Estimator:\n","     |  \n","     |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n","     |      Fits a model to the input dataset with optional parameters.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      params : dict or list or tuple, optional\n","     |          an optional param map that overrides embedded params. If a list/tuple of\n","     |          param maps is given, this calls fit on each param map and returns a list of\n","     |          models.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n","     |          fitted model(s)\n","     |  \n","     |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n","     |      Fits a model to the input dataset for each param map in `paramMaps`.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dataset : :py:class:`pyspark.sql.DataFrame`\n","     |          input dataset.\n","     |      paramMaps : :py:class:`collections.abc.Sequence`\n","     |          A Sequence of param maps.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :py:class:`_FitMultipleIterator`\n","     |          A thread safe iterable which contains one model for each param map. Each\n","     |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n","     |          using `paramMaps[index]`. `index` values may not be sequential.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  getProbabilityCol(self) -> str\n","     |      Gets the value of probabilityCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n","     |  \n","     |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  getThresholds(self) -> List[float]\n","     |      Gets the value of thresholds or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n","     |  \n","     |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  getRawPredictionCol(self) -> str\n","     |      Gets the value of rawPredictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n","     |  \n","     |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  getLabelCol(self) -> str\n","     |      Gets the value of labelCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n","     |  \n","     |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  getFeaturesCol(self) -> str\n","     |      Gets the value of featuresCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n","     |  \n","     |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  getPredictionCol(self) -> str\n","     |      Gets the value of predictionCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n","     |  \n","     |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._RandomForestParams:\n","     |  \n","     |  getBootstrap(self) -> bool\n","     |      Gets the value of bootstrap or its default value.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  getNumTrees(self) -> int\n","     |      Gets the value of numTrees or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._RandomForestParams:\n","     |  \n","     |  bootstrap = Param(parent='undefined', name='bootstrap', doc=...bootstr...\n","     |  \n","     |  numTrees = Param(parent='undefined', name='numTrees', doc='Number of t...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  getFeatureSubsetStrategy(self) -> str\n","     |      Gets the value of featureSubsetStrategy or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  getSubsamplingRate(self) -> float\n","     |      Gets the value of subsamplingRate or its default value.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeEnsembleParams:\n","     |  \n","     |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n","     |  \n","     |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n","     |  \n","     |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  getCacheNodeIds(self) -> bool\n","     |      Gets the value of cacheNodeIds or its default value.\n","     |  \n","     |  getLeafCol(self) -> str\n","     |      Gets the value of leafCol or its default value.\n","     |  \n","     |  getMaxBins(self) -> int\n","     |      Gets the value of maxBins or its default value.\n","     |  \n","     |  getMaxDepth(self) -> int\n","     |      Gets the value of maxDepth or its default value.\n","     |  \n","     |  getMaxMemoryInMB(self) -> int\n","     |      Gets the value of maxMemoryInMB or its default value.\n","     |  \n","     |  getMinInfoGain(self) -> float\n","     |      Gets the value of minInfoGain or its default value.\n","     |  \n","     |  getMinInstancesPerNode(self) -> int\n","     |      Gets the value of minInstancesPerNode or its default value.\n","     |  \n","     |  getMinWeightFractionPerNode(self) -> float\n","     |      Gets the value of minWeightFractionPerNode or its default value.\n","     |  \n","     |  setLeafCol(self: 'P', value: str) -> 'P'\n","     |      Sets the value of :py:attr:`leafCol`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n","     |  \n","     |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n","     |  \n","     |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n","     |  \n","     |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n","     |  \n","     |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n","     |  \n","     |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n","     |  \n","     |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n","     |  \n","     |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n","     |  \n","     |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  getCheckpointInterval(self) -> int\n","     |      Gets the value of checkpointInterval or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n","     |  \n","     |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  getSeed(self) -> int\n","     |      Gets the value of seed or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n","     |  \n","     |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  getWeightCol(self) -> str\n","     |      Gets the value of weightCol or its default value.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n","     |  \n","     |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  getImpurity(self) -> str\n","     |      Gets the value of impurity or its default value.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes inherited from pyspark.ml.tree._TreeClassifierParams:\n","     |  \n","     |  impurity = Param(parent='undefined', name='impurity', doc='...-insensi...\n","     |  \n","     |  supportedImpurities = ['entropy', 'gini']\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.param.Params:\n","     |  \n","     |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n","     |      Explains a single param and returns its name, doc, and optional\n","     |      default value and user-supplied value in a string.\n","     |  \n","     |  explainParams(self) -> str\n","     |      Returns the documentation of all params with their optionally\n","     |      default values and user-supplied values.\n","     |  \n","     |  extractParamMap(self, extra: Union[ForwardRef('ParamMap'), NoneType] = None) -> 'ParamMap'\n","     |      Extracts the embedded default param values and user-supplied\n","     |      values, and then merges them with extra values from input into\n","     |      a flat param map, where the latter value is used if there exist\n","     |      conflicts, i.e., with ordering: default param values <\n","     |      user-supplied values < extra.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      extra : dict, optional\n","     |          extra param values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          merged param map\n","     |  \n","     |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n","     |      Gets the value of a param in the user-supplied param map or its\n","     |      default value. Raises an error if neither is set.\n","     |  \n","     |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n","     |      Gets a param by its name.\n","     |  \n","     |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param has a default value.\n","     |  \n","     |  hasParam(self, paramName: str) -> bool\n","     |      Tests whether this instance contains a param with a given\n","     |      (string) name.\n","     |  \n","     |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user or has\n","     |      a default value.\n","     |  \n","     |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n","     |      Checks whether a param is explicitly set by user.\n","     |  \n","     |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n","     |      Sets a parameter in the embedded param map.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from pyspark.ml.param.Params:\n","     |  \n","     |  params\n","     |      Returns all params ordered by name. The default implementation\n","     |      uses :py:func:`dir` to get all attributes of type\n","     |      :py:class:`Param`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.Identifiable:\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n","     |  \n","     |  write(self) -> pyspark.ml.util.JavaMLWriter\n","     |      Returns an MLWriter instance for this ML instance.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from pyspark.ml.util.MLWritable:\n","     |  \n","     |  save(self, path: str) -> None\n","     |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n","     |  \n","     |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n","     |      Returns an MLReader instance for this class.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from pyspark.ml.util.MLReadable:\n","     |  \n","     |  load(path: str) -> ~RL from abc.ABCMeta\n","     |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params) from abc.ABCMeta\n","     |  \n","     |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from typing.Generic:\n","     |  \n","     |  __new__(cls, *args, **kwds)\n","     |      Create and return a new object.  See help(type) for accurate signature.\n","\n","DATA\n","    __all__ = ['LinearSVC', 'LinearSVCModel', 'LinearSVCSummary', 'LinearS...\n","\n","FILE\n","    /usr/local/lib/python3.8/dist-packages/pyspark/ml/classification.py\n","\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DfvVF1xI3mrA"},"execution_count":null,"outputs":[]}]}